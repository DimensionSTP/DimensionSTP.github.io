---
layout: single
title:  "LLM Train Recipe"
categories: Study-concept
tag: [LLM, Train Recipe, Tulu, SmolLM2, Dataset, Fine-tuning]
toc: true
author_profile: false
sidebar:
    nav: "docs"
search: true
typora-root-url: ../
---





#  LLM Train Recipe

ğŸ‘‰ğŸ»[Tulu3 paper link](https://arxiv.org/pdf/2411.15124)

ğŸ‘‰ğŸ»[SmolLM2 paper link](https://arxiv.org/html/2502.02737v1)

ìµœê·¼ AI ì—°êµ¬ ì»¤ë®¤ë‹ˆí‹°ì—ì„œëŠ” ëª¨ë“  í•™ìŠµ ë°ì´í„°ì™€ íŒŒë¼ë¯¸í„°ë¥¼ íˆ¬ëª…í•˜ê²Œ ê³µê°œí•˜ëŠ” ëª¨ë¸ë“¤ì´ ì£¼ëª©ë°›ê³  ìˆë‹¤.
**TÃ¼lu 3**ëŠ” Llama 3.1 ê¸°ë°˜ì˜ post-trained modelsë¡œ, base ëª¨ë¸ì„ ê°€ì ¸ì™€ì„œ ì§€ë„í•™ìŠµ(SFT), Preference íŠœë‹(DPO), ê·¸ë¦¬ê³  (8B ëª¨ë¸ì— í•œì •í•œ) ê°•í™”í•™ìŠµ(RLVR) ë‹¨ê³„ë¥¼ í†µí•´ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í–ˆë‹¤.
**SmolLM2**ëŠ” Pre-trainingë¶€í„° Fine-tuningê¹Œì§€ ì „ ê³¼ì •ì„ ì˜¤í”ˆ ë°ì´í„°ë¡œ ì§„í–‰í•œ ëª¨ë¸ë¡œ, Pre-training ë°ì´í„°ì™€ Fine-tuning ë°ì´í„°ê°€ ëª¨ë‘ ê³µê°œë˜ì–´ ìˆëŠ” ì´ˆê²½ëŸ‰ ì˜ì–´ ëª¨ë¸ì´ë‹¤.
ë³¸ ë¦¬ë·°ì—ì„œëŠ” ë‘ ëª¨ë¸ì˜ ì£¼ìš” êµ¬ì„± ìš”ì†Œë¥¼ ì„¸ì„¸í•˜ê²Œ ì‚´í´ë³´ê³ , ê° ë‹¨ê³„ë§ˆë‹¤ dataset, method, hyper-parametersë¥¼ ë¹„êµí•´ ë³¸ë‹¤.



# Model Overviews

**TÃ¼lu 3**ì™€ **SmolLM2**ì˜ ëª¨ë¸ êµ¬ì¡°ì— ëŒ€í•´ ê°„ëµíˆ ì •ë¦¬í•´ë³¸ë‹¤.



## TÃ¼lu 3

- **Base models:**
  - Llama 3.1 ê¸°ë°˜
  - Model parameters: 8B, 70B, 405B

- **Key points:**

  - ìì²´ êµ¬ì¶•í•œ post-train datasetìœ¼ë¡œ SFT, DPO, RLVRì„ ë‹¨ê³„ë³„ë¡œ ì ìš©
  - ê° post-train ë‹¨ê³„ì—ì„œì˜ hyper-parameters ë° reconstruction script ê³µê°œ

  - ê° ë‹¨ê³„ë³„ë¡œ ì²´í¬í¬ì¸íŠ¸ê°€ ê³µê°œë˜ì–´ ìˆì–´, ì—°êµ¬ìë“¤ì´ ì‰½ê²Œ ì¬í˜„ ë° í™•ì¥ì´ ê°€ëŠ¥



## SmolLM2

- **Base models:**
  - Llama(>3.1) Architectures ê¸°ë°˜
  - Model parameters: 135M, 360M, 1.7B
  - Pre-trainë¶€í„° ì§„í–‰

- **Key points:**
  - ìì²´ êµ¬ì¶•í•œ pre-train, post-train datasetìœ¼ë¡œ í•™ìŠµ
  - ê° í•™ìŠµ ë‹¨ê³„ì—ì„œì˜ hyper-parameters ê³µê°œ




# Data Overviews

**TÃ¼lu 3**ì™€ **SmolLM2**ì˜ í•™ìŠµ ë°ì´í„°ì…‹ì— ëŒ€í•´ ì•Œì•„ë³¸ë‹¤.



## TÃ¼lu 3

- **Supervised Fine-Tuning (SFT) ë°ì´í„°:**
  - ê³µê°œ ì¸ìŠ¤íŠ¸ëŸ­ì…˜ ë°ì´í„°ì…‹ê³¼ synthetic ë°ì´í„°ì˜ í˜¼í•©
  - ë‹¤ì–‘í•œ íƒœìŠ¤í¬(ëŒ€í™”, ìˆ˜í•™, ì½”ë”©, ì•ˆì „ì„± ë“±)ë¥¼ í¬í•¨
  - ğŸ‘‰ğŸ»[allenai/tulu-3-sft-mixture](https://huggingface.co/datasets/allenai/tulu-3-sft-mixture)

- **Preference Tuning(DPO) ë°ì´í„°:**
  - SFT ë‹¨ê³„ì˜ ì¶œë ¥ê³¼ íƒ€ ëª¨ë¸ì˜ ì‘ë‹µì„ ë¹„êµí•˜ì—¬ êµ¬ì„±ëœ on-policy ë°ì´í„°
  - ğŸ‘‰ğŸ»[allenai/llama-3.1-tulu-3-405b-preference-mixture](https://huggingface.co/datasets/allenai/llama-3.1-tulu-3-405b-preference-mixture)
  - ğŸ‘‰ğŸ»[allenai/llama-3.1-tulu-3-70b-preference-mixture](https://huggingface.co/datasets/allenai/llama-3.1-tulu-3-70b-preference-mixture)
  - ğŸ‘‰ğŸ»[allenai/llama-3.1-tulu-3-8b-preference-mixture](https://huggingface.co/datasets/allenai/llama-3.1-tulu-3-8b-preference-mixture)

- **RLVR(PPO) ë°ì´í„° (8B ëª¨ë¸ ì „ìš©):**
  - GSM8K, MATH, IFEval ë“± â€œê²€ì¦ ê°€ëŠ¥í•œ ì •ë‹µâ€ ì—¬ë¶€ì— ë”°ë¥¸ ë³´ìƒ ì²´ê³„ ë°ì´í„°
  - ğŸ‘‰ğŸ»[allenai/RLVR-GSM-MATH-IF-Mixed-Constraints](https://huggingface.co/datasets/allenai/RLVR-GSM-MATH-IF-Mixed-Constraints)



## SmolLM2

- **Pre-training ë°ì´í„°:**
  - **êµ¬ì„±:**
    - **FineWeb-Edu**: 1.3ì¡° í† í° ê·œëª¨ì˜ í•„í„°ë§ëœ êµìœ¡ ì½˜í…ì¸  ê¸°ë°˜ ì›¹ ë°ì´í„°ì…‹  
    - **DCLM (DataComp-LM baseline)**: 300ì¡°+ ì›ì‹œ(raw) í† í°ì—ì„œ í•„í„°ë§í•œ CommonCrawl ë°ì´í„°ì…‹  
    - **The Stack**: ì˜¤í”ˆì†ŒìŠ¤ ì½”ë“œ ë°ì´í„°ì…‹  
    - **FineMath**: ìƒˆë¡­ê²Œ êµ¬ì¶•í•œ ìˆ˜í•™ ë°ì´í„°ì…‹  
    - **Stack-Edu**: ì½”ë”© í•™ìŠµì„ ìœ„í•œ í•„í„°ë§ëœ ì½”ë“œ ë°ì´í„°  
  - **ìš”ì•½**

| **Models**       | **Sources**                                       | **Amounts** |
|------------------|---------------------------------------------------|-------------|
| **SmolLM2-135M** | FineWeb-Edu, DCLM, The Stack, FineMath, Stack-Edu | ~2T tokens  |
| **SmolLM2-360M** | FineWeb-Edu, DCLM, The Stack, FineMath, Stack-Edu | ~4T tokens  |
| **SmolLM2-1.7B** | FineWeb-Edu, DCLM, The Stack, FineMath, Stack-Edu | ~11T tokens |

- **Supervised Fine-Tuning (SFT) ë°ì´í„°:**
  - **êµ¬ì„±:**
    - **SmolTalk**: LLaMA 3.1 ê¸°ë°˜ì˜ Instruction-Tuning ë°ì´í„°ì…‹ìœ¼ë¡œ, ê³µê°œ ë°ì´í„° ë° ìì²´ ìƒì„± ë°ì´í„°(Smol-Magpie-Ultra, Smol-Rewrite ë“±)ë¥¼ í¬í•¨  
    - **UltraFeedback**: GPT-4ê°€ ì±„ì í•œ 64k í”„ë¡¬í”„íŠ¸ ë° ì„ í˜¸ë„ ë¹„êµ ë°ì´í„°ë¥¼ í¬í•¨í•˜ì—¬ í•™ìŠµ
  - **ìš”ì•½**

| **Models**       | **SFT(Supervised Fine-Tuning)**       | **DPO(Direct Preference Optimization)** | **Sources**            |
|------------------|---------------------------------------|-----------------------------------------|------------------------|
| **SmolLM2-135M** | SmolTalk(filtered, ì•½ 0.5M ê°œ samples) | UltraFeedback (~61k prompts pair)      | SmolTalk, UltraFeedback |
| **SmolLM2-360M** | SmolTalk(filtered, ì•½ 0.5M ê°œ samples) | UltraFeedback (~61k prompts pair)      | SmolTalk, UltraFeedback |
| **SmolLM2-1.7B** | SmolTalk(entire, ì•½ 1.1M ê°œ)           | UltraFeedback (~61k prompts pair)      | SmolTalk, UltraFeedback |

  - **ì¶”ê°€ ì •ë³´:**
    - ë°ì´í„°ì…‹ ì†ŒìŠ¤ ë° ì •ì œ ê´€ë ¨ ì„¸ë¶€ ì •ë³´ëŠ” ë¦¬í¬íŠ¸ ë¶€ë¡ ë° ê³µê°œ ì €ì¥ì†Œì—ì„œ í™•ì¸ ê°€ëŠ¥
    - ğŸ‘‰ğŸ»[smollm2 Dataset Repository](https://github.com/smollm2/dataset)




## í•™ìŠµ ë°ì´í„° ë¹„êµ ë° ê³µí†µì 

- **ê³µê°œì„±:**
  - ë‘ ëª¨ë¸ ëª¨ë‘ ëª¨ë“  ë°ì´í„°ì…‹ì„ ê³µê°œí•˜ì—¬, ì—°êµ¬ìë“¤ì´ ë™ì¼ ì¡°ê±´ì—ì„œ ì‹¤í—˜ ì¬í˜„ ë° ì»¤ìŠ¤í„°ë§ˆì´ì§•ì´ ê°€ëŠ¥í•˜ë„ë¡ ì§€ì›

- **ë°ì´í„° ë¯¹ì‹± ì „ëµ:**

  - TÃ¼lu 3ëŠ” í¬ìŠ¤íŠ¸ íŠ¸ë ˆì´ë‹ì„ ìœ„í•œ ë‹¨ê³„ë³„ ë°ì´í„° íë ˆì´ì…˜ì— ì§‘ì¤‘

  - SmolLM2ëŠ” ëŒ€ê·œëª¨ Pre-training ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì–¸ì–´ ëª¨ë¸ì˜ ë²”ìš©ì„±ì„ ê·¹ëŒ€í™”í•œ í›„, íƒœìŠ¤í¬ë³„ Fine-tuning ë°ì´í„°ë¡œ ì„±ëŠ¥ì„ ë³´ì™„

- **ëª©í‘œ:**
  - ë‘ ëª¨ë¸ ëª¨ë‘ ë‹¤ì–‘í•œ íƒœìŠ¤í¬ë¥¼ ì»¤ë²„í•˜ê¸° ìœ„í•´, ì—¬ëŸ¬ ë„ë©”ì¸ê³¼ ë¬¸ì²´ì˜ ë°ì´í„°ë¥¼ í˜¼í•©í•˜ëŠ” ì „ëµì„ ì·¨í•¨



# Train Pipelines & Hyper-Parameters

**TÃ¼lu 3**ì™€ **SmolLM2**ì˜ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ë° í•˜ì´í¼ íŒŒë¼ë¯¸í„°ì— ëŒ€í•´ ì•Œì•„ë³¸ë‹¤.



## TÃ¼lu 3

ğŸ‘‰ğŸ»[TÃ¼lu 3 scripts](https://github.com/allenai/open-instruct/blob/main/docs/tulu3.md)

 **TÃ¼lu 3**ì˜ ê²½ìš° ìœ„ ë§í¬ì— ê° ëª¨ë¸ íŒŒë¼ë¯¸í„°, ê·¸ë¦¬ê³  ë‹¨ê³„ë³„ reproduction scriptê°€ ìì„¸íˆ ë‚˜ì™€ìˆìœ¼ë©°, ì•„ë˜ ë‚´ìš©ì€ í•´ë‹¹ ë§í¬ì˜ ë‚´ìš©ì„ ì˜®ê²¨ ì ì€ ê²ƒì´ë‹¤.



### SFT

**Llama-3.1-Tulu-3-8B-SFT Reproduction**

Below is (almost) the exact command which produced [Llama-3.1-Tulu-3-8B-SFT](https://huggingface.co/allenai/Llama-3.1-Tulu-3-8B-SFT). We deployed the command across 8 machines, each equipped with 8 NVIDIA H100 GPUs, for a total of 64 GPUs in the our setup.

```bash
# modify the following `MACHINE_RANK`, `MAIN_PROCESS_IP`,
# `NUM_MACHINES`, `NUM_PROCESSES`, `PER_DEVICE_TRAIN_BATCH_SIZE`,
# `GRADIENT_ACCUMULATION_STEPS` according to your setup
MACHINE_RANK=0
MAIN_PROCESS_IP=localhost
NUM_MACHINES=8
NUM_PROCESSES=64
PER_DEVICE_TRAIN_BATCH_SIZE=1
GRADIENT_ACCUMULATION_STEPS=2
accelerate launch \
    --mixed_precision bf16 \
    --num_machines 8 \
    --num_processes 64 \
    --machine_rank $MACHINE_RANK \
    --main_process_ip $MAIN_PROCESS_IP \
    --main_process_port 29400 \
    --use_deepspeed \
    --deepspeed_config_file configs/ds_configs/stage3_no_offloading_accelerate.conf \
    --deepspeed_multinode_launcher standard open_instruct/finetune.py \
    --model_name_or_path meta-llama/Llama-3.1-8B \
    --tokenizer_name meta-llama/Llama-3.1-8B \
    --use_slow_tokenizer \
    --use_flash_attn \
    --max_seq_length 4096 \
    --preprocessing_num_workers 128 \
    --per_device_train_batch_size $PER_DEVICE_TRAIN_BATCH_SIZE \
    --gradient_accumulation_steps $GRADIENT_ACCUMULATION_STEPS \
    --learning_rate 5e-06 \
    --lr_scheduler_type linear \
    --warmup_ratio 0.03 \
    --weight_decay 0.0 \
    --num_train_epochs 2 \
    --output_dir output/sft_8b \
    --with_tracking \
    --report_to wandb \
    --logging_steps 1 \
    --reduce_loss sum \
    --model_revision main \
    --dataset_mixer_list allenai/tulu-3-sft-mixture 1.0 \
    --checkpointing_steps epoch \
    --dataset_mix_dir output/sft_8b \
    --exp_name tulu-3-8b-sft \
    --seed 123
# For Ai2 internal members, this was the experiment URL: https://beaker.org/ex/01JBNTPW8TKG09B2XR832YB5S8
```



**Llama-3.1-Tulu-3-70B-SFT Reproduction**

This is (almost) the exact command which produced [allenai/Llama-3.1-Tulu-3-70B-SFT](https://huggingface.co/allenai/Llama-3.1-Tulu-3-70B-SFT)

```bash
# modify the following `MACHINE_RANK`, `MAIN_PROCESS_IP`,
# `NUM_MACHINES`, `NUM_PROCESSES`, `PER_DEVICE_TRAIN_BATCH_SIZE`,
# `GRADIENT_ACCUMULATION_STEPS` according to your setup
MACHINE_RANK=0
MAIN_PROCESS_IP=localhost
NUM_MACHINES=8
NUM_PROCESSES=64
PER_DEVICE_TRAIN_BATCH_SIZE=1
GRADIENT_ACCUMULATION_STEPS=2
accelerate launch \
    --mixed_precision bf16 \
    --num_machines $NUM_MACHINES \
    --num_processes $NUM_PROCESSES \
    --machine_rank $MACHINE_RANK \
    --main_process_ip $MAIN_PROCESS_IP \
    --main_process_port 29400 \
    --use_deepspeed \
    --deepspeed_config_file configs/ds_configs/stage3_no_offloading_accelerate.conf \
    --deepspeed_multinode_launcher standard open_instruct/finetune.py \
    --model_name_or_path meta-llama/Llama-3.1-70B \
    --tokenizer_name meta-llama/Llama-3.1-70B \
    --use_slow_tokenizer \
    --use_flash_attn \
    --max_seq_length 4096 \
    --preprocessing_num_workers 128 \
    --per_device_train_batch_size $PER_DEVICE_TRAIN_BATCH_SIZE \
    --gradient_accumulation_steps $GRADIENT_ACCUMULATION_STEPS \
    --learning_rate 2e-06 \
    --lr_scheduler_type linear \
    --warmup_ratio 0.03 \
    --weight_decay 0.0 \
    --num_train_epochs 2 \
    --output_dir output/sft_70B \
    --with_tracking \
    --report_to wandb \
    --logging_steps 1 \
    --reduce_loss sum \
    --model_revision main \
    --dataset_mixer_list allenai/tulu-3-sft-mixture 1.0 \
    --dataset_mix_dir output/sft_70B \
    --checkpointing_steps 1000 \
    --keep_last_n_checkpoints 20 \
    --gradient_checkpointing \
    --exp_name tulu-3-70b-sft \
    --seed 456
# For Ai2 internal members, this was the experiment URL: https://beaker.org/ex/01JC5J4R80M18XQTDH47JSFRJY/
```



### DPO

**Llama-3.1-Tulu-3-8B-DPO Reproduction**

This is (almost) the exact command which produced [allenai/Llama-3.1-Tulu-3-8B-DPO](https://huggingface.co/allenai/Llama-3.1-Tulu-3-8B-DPO)

```bash
accelerate launch \
    --mixed_precision bf16 \
    --num_machines 1 \
    --num_processes 8 \
    --use_deepspeed \
    --deepspeed_config_file configs/ds_configs/stage3_no_offloading_accelerate.conf open_instruct/dpo_tune.py \
    --model_name_or_path allenai/Llama-3.1-Tulu-3-8B-SFT \
    --use_flash_attn \
    --tokenizer_name allenai/Llama-3.1-Tulu-3-8B-SFT \
    --max_seq_length 2048 \
    --preprocessing_num_workers 16 \
    --per_device_train_batch_size 1 \
    --gradient_accumulation_steps 16 \
    --learning_rate 5e-07 \
    --lr_scheduler_type linear \
    --warmup_ratio 0.1 \
    --weight_decay 0.0 \
    --num_train_epochs 1 \
    --output_dir output/dpo_8b \
    --with_tracking \
    --report_to wandb \
    --logging_steps 1 \
    --model_revision main \
    --gradient_checkpointing \
    --dataset_mixer_list allenai/llama-3.1-tulu-3-8b-preference-mixture 1.0 \
    --use_slow_tokenizer \
    --use_lora False \
    --dpo_loss_type dpo_norm \
    --dpo_beta 5 \
    --checkpointing_steps 1000 \
    --exp_name tulu-3-8b-dpo
# For Ai2 internal members, this was the experiment URL: https://beaker.org/ex/01JCRXP0AR5312S8MD3XGCN0J7/
```



**Llama-3.1-Tulu-3-70B-DPO Reproduction**

This is (almost) the exact command which produced [allenai/Llama-3.1-Tulu-3-70B-DPO](https://huggingface.co/allenai/Llama-3.1-Tulu-3-70B-DPO)

```bash
# modify the following `MACHINE_RANK`, `MAIN_PROCESS_IP`,
# `NUM_MACHINES`, `NUM_PROCESSES`, `PER_DEVICE_TRAIN_BATCH_SIZE`,
# `GRADIENT_ACCUMULATION_STEPS` according to your setup
MACHINE_RANK=0
MAIN_PROCESS_IP=localhost
NUM_MACHINES=8
NUM_PROCESSES=64
PER_DEVICE_TRAIN_BATCH_SIZE=1
GRADIENT_ACCUMULATION_STEPS=2
accelerate launch \
    --mixed_precision bf16 \
    --num_machines $NUM_MACHINES \
    --num_processes $NUM_PROCESSES \
    --machine_rank $MACHINE_RANK \
    --main_process_ip $MAIN_PROCESS_IP \
    --main_process_port 29400 \
    --use_deepspeed \
    --deepspeed_config_file configs/ds_configs/stage3_offloading_accelerate.conf \
    --deepspeed_multinode_launcher standard open_instruct/dpo_tune_cache.py \
    --model_name_or_path allenai/Llama-3.1-Tulu-3-70B-SFT \
    --tokenizer_name allenai/Llama-3.1-Tulu-3-70B-SFT \
    --use_flash_attn \
    --max_seq_length 2048 \
    --preprocessing_num_workers 16 \
    --per_device_train_batch_size $PER_DEVICE_TRAIN_BATCH_SIZE \
    --gradient_accumulation_steps $GRADIENT_ACCUMULATION_STEPS \
    --learning_rate 2e-07 \
    --lr_scheduler_type linear \
    --warmup_ratio 0.1 \
    --weight_decay 0.0 \
    --num_train_epochs 1 \
    --output_dir output/dpo_70b \
    --with_tracking \
    --report_to wandb \
    --logging_steps 1 \
    --model_revision main \
    --gradient_checkpointing \
    --dataset_mixer_list allenai/llama-3.1-tulu-3-70b-preference-mixture \
    --use_slow_tokenizer \
    --use_lora False \
    --dpo_loss_type dpo_norm \
    --dpo_beta 5 \
    --checkpointing_steps epoch \
    --exp_name tulu-3-70b-dpo
# For Ai2 internal members, this was the experiment URL: https://beaker.org/ex/01JCSAYYHQYF9QDQDCV6KJ53M9/
```



**Llama-3.1-Tulu-3-405B-DPO Reproduction**

This is (almost) the exact command which produced [allenai/Llama-3.1-Tulu-3-405B-DPO](https://huggingface.co/allenai/Llama-3.1-Tulu-3-405B-DPO)

```bash
# modify the following `MACHINE_RANK`, `MAIN_PROCESS_IP`,
# `NUM_MACHINES`, `NUM_PROCESSES`, `PER_DEVICE_TRAIN_BATCH_SIZE`,
# `GRADIENT_ACCUMULATION_STEPS` according to your setup
MACHINE_RANK=0
MAIN_PROCESS_IP=localhost
NUM_MACHINES=8
NUM_PROCESSES=64
PER_DEVICE_TRAIN_BATCH_SIZE=1
GRADIENT_ACCUMULATION_STEPS=2
accelerate launch --mixed_precision bf16 \
    --num_machines 32 \
    --num_processes 256 \
    --machine_rank $BEAKER_REPLICA_RANK \
    --main_process_ip $BEAKER_LEADER_REPLICA_HOSTNAME \
    --main_process_port 29400 \
    --use_deepspeed \
    --deepspeed_config_file configs/ds_configs/stage3_no_offloading_accelerate.conf \
    --deepspeed_multinode_launcher standard open_instruct/dpo_tune_cache.py \
    --model_name_or_path allenai/Llama-3.1-Tulu-3-405B-SFT \
    --tokenizer_name allenai/Llama-3.1-Tulu-3-70B-SFT \
    --use_flash_attn \
    --max_seq_length 2048 \
    --preprocessing_num_workers 16 \
    --per_device_train_batch_size 1 \
    --gradient_accumulation_steps 1 \
    --learning_rate 2e-07 \
    --lr_scheduler_type linear \
    --warmup_ratio 0.1 \
    --weight_decay 0.0 \
    --num_train_epochs 1 \
    --output_dir output_405b \
    --with_tracking \
    --report_to wandb \
    --logging_steps 1 \
    --model_revision main \
    --gradient_checkpointing \
    --dataset_mixer_list ai2-adapt-dev/405b_preference_mix 1.0 \
    --use_slow_tokenizer \
    --use_lora False \
    --dpo_loss_type dpo_norm \
    --dpo_beta 5 \
    --checkpointing_steps 1000
# For Ai2 internal members, this was the experiment URL: https://beaker.org/ex/01JJ4QRZ31SH79AHVM6WWDVJB4/
```



### RLVR

**Llama-3.1-Tulu-3-8B-RM Reproduction**

This is (almost) the exact command which produced [allenai/Llama-3.1-Tulu-3-8B-RM](https://huggingface.co/allenai/Llama-3.1-Tulu-3-8B-RM)

```bash
accelerate launch \
    --config_file configs/ds_configs/deepspeed_zero3.yaml open_instruct/reward_modeling.py \
    --dataset_mixer '{"allenai/llama-3.1-tulu-3-8b-preference-mixture": 1.0}' \
    --dataset_train_splits train \
    --dataset_eval_mixer '{"allenai/ultrafeedback_binarized_cleaned": 1.0}' \
    --dataset_eval_splits test_prefs \
    --model_name_or_path allenai/Llama-3.1-Tulu-3-8B-SFT \
    --chat_template tulu \
    --learning_rate 3e-6 \
    --per_device_train_batch_size 1 \
    --per_device_eval_batch_size 1 \
    --gradient_accumulation_steps 32 \
    --max_token_length 2048 \
    --max_prompt_token_length 2048 \
    --num_train_epochs 1 \
    --output_dir output/rm_8b \
    --gradient_checkpointing \
    --push_to_hub \
    --with_tracking
# For Ai2 internal members, this was the experiment URL: https://beaker.org/ex/01JCS01RFBQGFE5F1W3W96FFVM/
```



**Llama-3.1-Tulu-3-8B Reproduction**

This is (almost) the exact command which produced [allenai/Llama-3.1-Tulu-3-8B](https://huggingface.co/allenai/Llama-3.1-Tulu-3-8B)

```bash
python open_instruct/ppo_vllm_thread_ray_gtrl.py \
    --exp_name tulu-3-8b-rlvr \
    --dataset_mixer_list allenai/RLVR-GSM-MATH-IF-Mixed-Constraints 1.0 \
    --dataset_mixer_list_splits train \
    --dataset_mixer_eval_list allenai/RLVR-GSM-MATH-IF-Mixed-Constraints 16 \
    --dataset_mixer_eval_list_splits train \
    --max_token_length 2048 \
    --max_prompt_token_length 2048 \
    --response_length 2048 \
    --model_name_or_path allenai/Llama-3.1-Tulu-3-8B-DPO \
    --reward_model_path allenai/Llama-3.1-Tulu-3-8B-RM \
    --non_stop_penalty \
    --stop_token eos \
    --temperature 1.0 \
    --ground_truths_key ground_truth \
    --chat_template_name tulu \
    --sft_messages_key messages \
    --learning_rate 3e-7 \
    --total_episodes 10000000 \
    --penalty_reward_value -10.0 \
    --deepspeed_stage 3 \
    --per_device_train_batch_size 2 \
    --local_rollout_forward_batch_size 2 \
    --local_mini_batch_size 32 \
    --local_rollout_batch_size 32 \
    --actor_num_gpus_per_node 7 \
    --vllm_tensor_parallel_size 1 \
    --beta 0.05 \
    --apply_verifiable_reward true \
    --output_dir output/rlvr_8b \
    --seed 3 \
    --num_evals 3 \
    --save_freq 100 \
    --reward_model_multiplier 0.0 \
    --gradient_checkpointing \
    --with_tracking
# For Ai2 internal members, this was the experiment URL: https://beaker.org/ex/01JCVTA10BQDVGGQKFYWEZ6KCQ/
```



**Llama-3.1-Tulu-3-70B Reproduction**

This is (almost) the exact command which produced [allenai/Llama-3.1-Tulu-3-70B](https://huggingface.co/allenai/Llama-3.1-Tulu-3-70B)

Couple of notes:
* Make sure to modify `configs/beaker_configs/ray_node_setup.sh` in our own cluster setup. The idea is to have the replicas join the main machines via `ray`.
* We had to use `--vllm_tensor_parallel_size 4` because `--vllm_tensor_parallel_size 8` errors out for some strange reason. This is a temporary workaround.
* Here the effective batch size is `sum(actor_num_gpus_per_node) * local_mini_batch_size = 40 * 16 = 640`. If you have less GPUs, you can adjust `actor_num_gpus_per_node` and `local_mini_batch_size` accordingly.

```bash
source configs/beaker_configs/ray_node_setup.sh && python open_instruct/ppo_vllm_thread_ray_gtrl.py \
    --dataset_mixer_list allenai/RLVR-GSM-MATH-IF-Mixed-Constraints 1.0 \
    --dataset_mixer_list_splits train \
    --dataset_mixer_eval_list allenai/RLVR-GSM-MATH-IF-Mixed-Constraints 16 \
    --dataset_mixer_eval_list_splits train \
    --max_token_length 2048 \
    --max_prompt_token_length 2048 \
    --response_length 2048 \
    --model_name_or_path allenai/Llama-3.1-Tulu-3-70B-DPO \
    --exp_name tulu-3-70b-rlvr \
    --reward_model_path allenai/Llama-3.1-Tulu-3-8B-RM \
    --beta 0.07 \
    --warmup_ratio 0.1 \
    --seed 8 \
    --output_dir output/rlvr_70b \
    --non_stop_penalty \
    --stop_token eos \
    --temperature 1.0 \
    --ground_truths_key ground_truth \
    --chat_template_name tulu \
    --sft_messages_key messages \
    --learning_rate 1e-7 \
    --total_episodes 400000 \
    --penalty_reward_value -10.0 \
    --deepspeed_stage 3 \
    --per_device_train_batch_size 1 \
    --local_rollout_forward_batch_size 1 \
    --local_mini_batch_size 16 \
    --local_rollout_batch_size 16 \
    --actor_num_gpus_per_node 8 8 8 8 8 \
    --vllm_num_engines 1 \
    --vllm_tensor_parallel_size 4 \
    --apply_verifiable_reward true \
    --reward_model_multiplier 0.0 \
    --no_gather_whole_model \
    --num_evals 3 \
    --save_freq 40 \
    --gradient_checkpointing \
    --with_tracking
# For Ai2 internal members, this was the experiment URL: https://beaker.org/ex/01JD3YEM4XGH2F2H10Y49GK441/
```



**Llama-3.1-Tulu-3-405B Reproduction**

This is (almost) the exact command which produced [allenai/Llama-3.1-Tulu-3-405B](https://huggingface.co/allenai/Llama-3.1-Tulu-3-405B)

Couple of notes:
* We had to set `TORCH_NCCL_ENABLE_MONITORING=0` to turn off NCCL heartbeat monitoring and avoid timeouts. Feel free to remove this.
* Make sure to modify `configs/beaker_configs/ray_node_setup.sh` in our own cluster setup. The idea is to have the replicas join the main machines via `ray`.
* Here the effective batch size is `sum(actor_num_gpus_per_node) * local_mini_batch_size = 40 * 16 = 640`. If you have less GPUs, you can adjust `actor_num_gpus_per_node` and `local_mini_batch_size` accordingly.

```bash
TORCH_NCCL_ENABLE_MONITORING=0 python mason.py \
    --cluster ai2/jupiter-cirrascale-2 --pure_docker_mode \
    --workspace ai2/tulu-3-dev \
    --priority urgent \
    --preemptible \
    --num_nodes 32 \
    --image nathanl/open_instruct_auto \
    --budget ai2/oe-adapt \
    --gpus 8 -- source configs/beaker_configs/ray_node_setup.sh \&\& TORCH_DISTRIBUTED_DEBUG=DETAIL python open_instruct/ppo_vllm_thread_ray_gtrl.py \
    --dataset_mixer_list allenai/RLVR-MATH 1.0 \
    --dataset_mixer_list_splits train \
    --dataset_mixer_eval_list allenai/RLVR-MATH 128 \
    --dataset_mixer_eval_list_splits train \
    --max_token_length 2048 \
    --max_prompt_token_length 2048 \
    --response_length 1024 \
    --model_name_or_path /weka/oe-adapt-default/hamishi/405b_dpo_v4 \
    --exp_name "405b_rlvr_math_only_8b_valu_on_v4" \
    --reward_model_path allenai/Llama-3.1-Tulu-3-8B-RM \
    --beta 0.05 \
    --output_dir "/weka/oe-adapt-default/hamishi/405b_rlvr_math_only_8b_valu_on_v4" \
    --non_stop_penalty \
    --stop_token eos \
    --temperature 1.0 \
    --ground_truths_key ground_truth \
    --chat_template tulu \
    --sft_messages_key messages \
    --learning_rate 1e-7 \
    --total_episodes 400000 \
    --num_epochs 4 \
    --penalty_reward_value -10.0 \
    --deepspeed_stage 3 \
    --per_device_train_batch_size 1 \
    --local_rollout_forward_batch_size 1 \
    --local_mini_batch_size 8 \
    --local_rollout_batch_size 8 \
    --actor_num_gpus_per_node 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 \
    --vllm_num_engines 1 \
    --vllm_tensor_parallel_size 16 \
    --vllm_enforce_eager true \
    --apply_verifiable_reward true \
    --reward_model_multiplier 0.0 \
    --no_gather_whole_model \
    --seed 3 \
    --num_evals 3 \
    --no_try_launch_beaker_eval_jobs \
    --save_freq 25 \
    --try_launch_beaker_eval_jobs_on_weka \
    --gradient_checkpointing \
    --with_tracking
# For Ai2 internal members, this was the experiment URL: https://beaker.org/ex/01JJA31S20XAFR82YPFKSMMYZV/
```



## SmolLM2

Pre-training, Fine-tuningì— ëŒ€í•´ í‘œë¡œ ì •ë¦¬í•´ë³¸ë‹¤.



### Pre-training

ëª¨ë“  SmolLM2 ëª¨ë¸ì€ **AdamW Optimizer**ì™€ **Warmupâ€“Stableâ€“Decay(WSD) LR Scheduler**ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì‹œì¼°ë‹¤.

- **ìš”ì•½**

| **Models** | **Trained Tokens** | **Optimizer** | **Learning Rate(Peak)** | **LR Scheduler**    | **Warmup Steps** | **Sequence Length** | **Global Batch Size** |
|------------|--------------------|---------------|-------------------------|---------------------|------------------|---------------------|-----------------------|
| **135M**   | ~2T                | AdamW         | ~3.0Ã—10^-3              | WSD (Stable, Decay) | 2000             | 2048                | 2M tokens             |
| **360M**   | ~4T                | AdamW         | ~3.0Ã—10^-3              | WSD (Stable, Decay) | 2000             | 2048                | 2M tokens             |
| **1.7B**   | ~11T               | AdamW         | ~5.0Ã—10^-4              | WSD (Stable, Decay) | 2000             | 2048                | 2M tokens             |

- **ì„¤ëª…**
  - AdamW (Î²â‚=0.9, Î²â‚‚=0.999) Optimizer ì‚¬ìš©  
  - LR Scheduling: **ì´ˆê¸° 2000 ìŠ¤í… Warmup â†’ ì¼ì • ìœ ì§€(Stable) â†’ ë§ˆì§€ë§‰ 10% Decay**  
  - 1.7B ëª¨ë¸ì€ **4ë‹¨ê³„ í•™ìŠµ(0-6T, 6-8T, 8-10T, 10-11T) ì§„í–‰**  
  - 135M ë° 360M ëª¨ë¸ì€ **ìˆ˜í•™ ë° ì½”ë”© ë°ì´í„°ê°€ í›„ë°˜ë¶€ì— ì§‘ì¤‘ì ìœ¼ë¡œ íˆ¬ì…ë¨**  



### Supervised Fine-Tuning(SFT)

SmolTalk ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ Instruction-Following ëŠ¥ë ¥ì„ í•™ìŠµì‹œì¼°ë‹¤.

- **ìš”ì•½**

| **Models** | **Trained Samples** | **Optimizer** | **Learning Rate(Peak)** | **LR Scheduler**    | **Epochs**  | **Sequence Length** | **Global Batch Size** |
|------------|---------------------|---------------|-------------------------|---------------------|-------------|---------------------|-----------------------|
| **135M**   | ~0.5M               | AdamW         | ~3.0Ã—10^-4              | WSD (Stable, Decay) | 2           | 8192                | 128                   |
| **360M**   | ~0.5M               | AdamW         | ~3.0Ã—10^-4              | WSD (Stable, Decay) | 2           | 8192                | 128                   |
| **1.7B**   | ~1.1M               | AdamW         | ~3.0Ã—10^-4              | WSD (Stable, Decay) | 2           | 8192                | 128                   |



### Direct Preference Optimization(DPO)

UltraFeedback ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ì¶œë ¥ì„ ì¸ê°„ ì„ í˜¸ë„ì— ë§ê²Œ ìµœì í™”ì‹œì¼°ë‹¤.

- **ìš”ì•½**

| **Models** | **DPO Pairs** | **Optimizer** | **Learning Rate(Peak)** | **LR Scheduler**    | **Epochs**  | **Sequence Length** | **Global Batch Size** | **DPO beta** |
|------------|---------------|---------------|-------------------------|---------------------|-------------|---------------------|-----------------------|--------------|
| **135M**   | ~61k          | AdamW         | ~1.0Ã—10^-6              | WSD (Stable, Decay) | 2           | 1024                | 128                   | 0.5          |
| **360M**   | ~61k          | AdamW         | ~1.0Ã—10^-6              | WSD (Stable, Decay) | 2           | 1024                | 128                   | 0.5          |
| **1.7B**   | ~61k          | AdamW         | ~1.0Ã—10^-6              | WSD (Stable, Decay) | 2           | 1024                | 128                   | 0.5          |



## ë¹„êµ ë° ì¸ì‚¬ì´íŠ¸

- **TÃ¼lu 3:**

  - Llama 3.1 ê¸°ë°˜ì˜ ëª¨ë¸ì— ëŒ€í•´ í›„ì²˜ë¦¬(post-training)ë¡œ SFT, Preference íŠœë‹, RLVR ë‹¨ê³„ë¥¼ ì ìš©

  - ëª¨ë¸ í¬ê¸°ì— ë”°ë¥¸ ì„¸ë°€í•œ íŒŒë¼ë¯¸í„° ì¡°ì •ì´ ë‹ë³´ì„

- **SmolLM2:**

  - Pre-trainingë¶€í„° ì‹œì‘í•˜ì—¬, ëŒ€ê·œëª¨ ë°ì´í„°ë¡œ ë²”ìš© ì–¸ì–´ ëª¨ë¸ì„ êµ¬ì¶•í•œ í›„ Fine-tuningìœ¼ë¡œ íŠ¹ì • íƒœìŠ¤í¬ì— ë§ì¶¤

  - Pre-training ë‹¨ê³„ì˜ ë°ì´í„° ê·œëª¨(ì•½ 50B í† í°)ì™€ ê¸´ í•™ìŠµ ìŠ¤í…ì´ íŠ¹ì§•

- **ê³µí†µì :**

  - ëª¨ë“  ë‹¨ê³„ì—ì„œ AdamW optimizer ì‚¬ìš©, warm up ë° weight decay ì ìš©

  - í•™ìŠµ ë°ì´í„°ì™€ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ê°€ ì „ ê³¼ì • ê³µê°œë˜ì–´ ìˆìŒ

  - ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° ì‚¬ì´ì¦ˆ, ê° í•™ìŠµ ë‹¨ê³„ì— ë”°ë¼ í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì¡°ì •ì´ ìˆìŒ



# Model Checkpoints Information

**TÃ¼lu 3**ì™€ **SmolLM2**ì˜ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ë“¤ì„ ê¸°ë¡í•¨.



## TÃ¼lu 3

- **8B:**

  - SFT: `allenai/Llama-3.1-Tulu-3-8B-SFT`

  - DPO: `allenai/Llama-3.1-Tulu-3-8B-DPO`

  - RLVR ìµœì¢…: `allenai/Llama-3.1-Tulu-3-8B`

  - ë³´ìƒ ëª¨ë¸(RM): `allenai/Llama-3.1-Tulu-3-8B-RM`

- **70B:**

  - SFT: `allenai/Llama-3.1-Tulu-3-70B-SFT`

  - DPO: `allenai/Llama-3.1-Tulu-3-70B-DPO`

  - ìµœì¢… ëª¨ë¸: `allenai/Llama-3.1-Tulu-3-70B`

- **405B:**

  - SFT: `allenai/Llama-3.1-Tulu-3-405B-SFT`

  - DPO: `allenai/Llama-3.1-Tulu-3-405B-DPO`
  
  - ìµœì¢… ëª¨ë¸: `allenai/Llama-3.1-Tulu-3-405B`



## SmolLM2

- **135M:**

  - Base: `HuggingFaceTB/SmolLM2-135M`

  - Instruct: `HuggingFaceTB/SmolLM2-135M-Instruct`

- **360M:**

  - Base: `HuggingFaceTB/SmolLM2-360M`

  - Instruct: `HuggingFaceTB/SmolLM2-360M-Instruct`

- **1.7B:**

  - Base: `HuggingFaceTB/SmolLM2-1.7B`

  - Instruct: `HuggingFaceTB/SmolLM2-1.7B-Instruct`




# ìš”ì•½ ë° ê²°ë¡ 

TÃ¼lu 3ì™€ SmolLM2 ëª¨ë‘ ë°ì´í„° íˆ¬ëª…ì„±ê³¼ ì„¸ë°€í•œ í•™ìŠµ ë ˆì‹œí”¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¬í˜„ ê°€ëŠ¥í•˜ê³  í™•ì¥ì„±ì´ ë›°ì–´ë‚œ ëª¨ë¸ êµ¬ì¶• ì‚¬ë¡€ë‹¤.

- **TÃ¼lu 3:** Llama 3.1 ê¸°ë°˜ í¬ìŠ¤íŠ¸ íŠ¸ë ˆì´ë‹ ëª¨ë¸ë¡œ, SFT, DPO, RLVR ë‹¨ê³„ë¥¼ í†µí•´ ìµœì¢… ëª¨ë¸ ì™„ì„±
- **SmolLM2:** Llama architecture ê¸°ë°˜ ëª¨ë¸ë¡œ, Pre-train, task ë³„ SFTë¥¼ í†µí•´ ìµœì¢… ëª¨ë¸ ì™„ì„±



ì•ìœ¼ë¡œë„ ì´ëŸ¬í•œ ê³µê°œ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ì–‘í•œ ì‹¤í—˜ê³¼ ì—°êµ¬ê°€ ì´ë£¨ì–´ì§€ê¸¸ ê¸°ëŒ€í•œë‹¤.

ë˜í•œ, ë‘ ëª¨ë¸ì˜ ë ˆì‹œí”¼ë¥¼ ì‹¤ì œ í•™ìŠµì— ì ìš©í•´ë³´ëŠ” ê²½í—˜ì´ í° ë„ì›€ì´ ë  ê²ƒì´ë¼ ë¯¿ëŠ”ë‹¤.

