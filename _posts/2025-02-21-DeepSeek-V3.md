---
layout: single
title:  "DeepSeek-V3 Review"
categories: Study-concept
tag: [DeepSeek, DeepSeek-V3, MLA, MoE, MTP, Mixed-precision, Hopper architecture]
toc: true
author_profile: false
sidebar:
    nav: "docs"
search: true
typora-root-url: ../
---





#  DeepSeek-V3 Review

👉🏻[논문 링크](https://arxiv.org/abs/2412.19437 "DeepSeek V3") 

DeepSeek-V3에 대한 리뷰는 이미 많이 나왔지만, 단순한 성능 비교나 벤치마크 결과를 나열하는 것을 넘어 **최적의 성능을 끌어내는 트랜스포머 아키텍처 설계와 효율적인 학습을 위한 혼합 정밀도 활용**에 집중하려 한다.

특히,

- 전통적인 디코더 기반 트랜스포머 모델에 다양한 기술을 적용해 **지속적인 개선**을 이루어 전반적인 효율 및 성능을 향상시킨 점
- Nvidia Hopper Architecture의 강점을 최대한 활용한 **FP8 Mixed Precision** 기법을 통해, 성능은 유지하면서도 자원 및 시간을 크게 절감한 점
- 학습 과정이 Pre-training, Post-training (SFT), Post-training (RL)으로 나뉘며, 각 단계별로 learning rate, 배치 사이즈, 데이터 출처 및 양 등이 체계적으로 조정된 점

이 세 가지 핵심 요소를 중심으로 DeepSeek-V3의 아키텍처와 학습 기법이 기존 모델과 어떤 차별점을 가지는지 살펴보자.



# Architectures

DeepSeek-V3의 아키텍처는 기존의 **전통적인 디코더 기반 트랜스포머 모델**과 비교했을 때 세 가지 중요한 차이점을 가진다.



### 1. Multi-head Latent Attention (MLA)

기존 어텐션 메커니즘은 모델 크기가 커질수록 연산량과 메모리 사용량이 증가하는 문제가 있었다. DeepSeek-V3는 이를 해결하기 위해 **MLA (Multi-head Latent Attention)**라는 자체 어텐션 구조를 도입했다. MLA는 기존 Multi-Head Attention (MHA)와 달리 **Q, K, V 행렬을 처음부터 LoRA (Low-Rank Adaptation) 방식으로 구성**하고, K와 V를 **하나의 projection에서 생성**하여 파라미터 효율성을 극대화한다. 또한, MLA는 기존 RoPE (Rotary Position Embedding)와 함께 **NoPE (No Position Embedding)** 방식을 병행하여 다양한 위치 인코딩 전략을 적용하며, GQA (Grouped Query Attention) 및 MQA (Multi-query Attention) 방식을 **혼합적으로 사용**해 연산 효율과 성능을 동시에 확보한다.



### 2. Mixture of Experts (MoE)

MoE (Mixture of Experts)는 기존에도 널리 활용되던 기법이지만, DeepSeek-V3는 이를 한 단계 발전시켰다. 전통적인 MoE에서는 전문가 네트워크 간의 부하 불균형을 조절하기 위해 auxiliary loss를 추가로 사용했으나, DeepSeek-V3는 **auxiliary-loss free strategy**를 도입해 별도의 손실 함수 없이도 전문가 간 부하가 자연스럽게 분산되도록 설계되었다. 또한, **기본 Top-p 방식의 전문가 선택 (default top-p strategy)** 을 채택하여 특정 전문가가 과도하게 활성화되는 문제를 완화하고, 데이터 특성에 따라 동적으로 전문가를 할당할 수 있다.



### 3. Multi-Token Prediction (MTP)

DeepSeek-V3는 최근 트랜스포머 기반 모델들이 학습 단계에서 도입하는 **Multi-Token Prediction (MTP)** 기법을 채택하였다. 기존 MTP 방식은 Medusa와 같이 여러 개의 LM Head를 배치해 각 Head가 개별적으로 토큰을 예측하는 형태였으나, DeepSeek-V3는 **LM Head를 공유**하면서 Transformer 블록 내에서 **재귀적으로 MTP를 적용**하는 방식을 도입해 파라미터 수 증가 없이 한 번의 forward pass에서 여러 토큰을 예측할 수 있도록 설계되었다. 단, 현재 공개된 추론 코드에는 해당 MTP 기법이 포함되어 있지 않아, 학습 과정에서의 구체적 적용 방식은 추가 검증이 필요하다.



이처럼, MLA, MoE, MTP 세 가지 핵심 기법 덕분에 DeepSeek-V3는 **동일한 파라미터 규모 내에서 더 나은 성능과 연산 효율**을 달성할 수 있다. 다음으로는 각 아키텍처에 대해 더욱 상세히 분석해보자.



## Multi-head Latent Attention(MLA)

DeepSeek-V3에서 가장 눈에 띄는 차별점 중 하나는 **Multi-head Latent Attention(MLA)**이다. 기존의 트랜스포머 모델에서 사용되는 **Multi-Head Attention(MHA)**는 각 헤드마다 독립적인 Key(K)와 Value(V)를 생성하는 방식이었지만, MLA는 이를 개선하여 **연산량과 메모리 사용량을 줄이면서도 성능을 유지하는 방식**을 도입했다.



### 1. MLA의 핵심 개념

일반적인 트랜스포머에서는 Q, K, V 행렬을 각 헤드마다 독립적으로 생성하지만, DeepSeek-V3의 MLA는 **LoRA(Low-Rank Adaptation) 기법을 활용하여 Q, K, V를 처음부터 경량화된 형태로 생성**한다. 특히, **K와 V를 하나의 projection에서 생성**하는 구조를 갖추고 있어, 파라미터 수를 줄이면서도 유사한 어텐션 성능을 유지할 수 있도록 설계되었다.


📌 *논문에서 MLA figure*

![MLA](/images/2025-02-21-DeepSeek-V3/deepseek-mla.png){: .align-center}



### 2. RoPE vs. NoPE: MLA에서의 위치 인코딩 방식

MLA는 **RoPE(Rotary Position Embedding)와 NoPE(No Position Embedding)를 동시에 지원**하는 구조를 갖추고 있다.

- **RoPE**는 기존 Llama 시리즈에서 사용된 방식으로, 상대적 위치 정보를 어텐션 메커니즘에 자연스럽게 주입할 수 있는 강점을 가진다.

- NoPE

  는 특정한 위치 인코딩을 사용하지 않고, MLA 자체의 학습 과정에서 자연스럽게 패턴을 익히도록 하는 방식이다.

  - MLA에서는 GQA(Grouped Query Attention) 및 MQA(Multi-Query Attention)를 **동시에 활용**할 수 있도록 설계되었으며, 이때 Key 헤드 차원(`k head dim`)이 1이면 **RoPE 또는 NoPE 방식 중 하나를 선택하여 적용**한다.



### 3. MLA와 기존 Llama 어텐션의 차이점

기존의 Llama 모델에서도 어텐션 최적화를 위한 여러 기법이 적용되었지만, MLA는 다음과 같은 추가적인 개선점을 포함하고 있다.

- **LoRA 기반의 경량 Q, K, V 생성**
- **K, V를 하나의 projection에서 추출하여 메모리 효율성 증가**
- **RoPE 및 NoPE 선택 가능 (GQA, MQA 동시 활용 가능)**



📌 *MLA 도식화*

![DeepSeek Attention](/images/2025-02-21-DeepSeek-V3/deepseek-attention.png){: .align-center}



📌 *General Attention(like Llama) 도식화*

![General Attention](/images/2025-02-21-DeepSeek-V3/general-attention.png){: .align-center}



### 4. MLA의 성능 및 기대 효과

이러한 설계를 통해 MLA는 기존 MHA 대비 **메모리 사용량을 줄이면서도 동일한 수준의 성능을 유지**하는 데 초점을 맞췄다. 특히, **LoRA를 활용한 Q, K, V 생성과 K, V의 단일 projection 사용으로 인해 연산량을 절감**할 수 있다는 점이 큰 장점이다.


📌 *MLA 공식 구현 코드(init)

![DeepSeek MLA init](/images/2025-02-21-DeepSeek-V3/deepseek-mla-code-init.png){: .align-center}



📌 *MLA 공식 구현 코드(forward)

![DeepSeek MLA init](/images/2025-02-21-DeepSeek-V3/deepseek-mla-code-forward.png){: .align-center}



결론적으로, MLA는 기존 어텐션 구조에서 발생하는 **메모리 및 연산량 문제를 효과적으로 개선하면서도 성능을 유지하는 독자적인 DeepSeek-V3의 핵심 아키텍처**라 할 수 있다.





## Mixture of Experts(MoE)

👉🏻[DeepSeek MoE 논문 링크](https://arxiv.org/pdf/2401.06066 "DeepSeek MoE") 

DeepSeek-V3는 기존에도 널리 사용되던 **Mixture of Experts (MoE)** 구조를 채택했지만, 단순한 MoE 모델이 아니라 **효율성과 균형을 최적화한 새로운 전략을 도입**했다. 일반적으로 MoE는 거대한 모델에서 연산량을 줄이면서도 성능을 유지할 수 있도록 하는 강력한 기법이지만, 전문가 네트워크 간의 **부하 불균형(Load Imbalance) 문제**와 **추가적인 auxiliary loss 필요성**이 단점으로 지적되어 왔다. DeepSeek-V3는 이러한 한계를 해결하기 위해 **auxiliary-loss free strategy**와 **default top-p gating** 기법을 도입했다.



### 1. MoE의 기본 개념

MoE는 여러 개의 전문가(Expert) 네트워크 중 일부만 활성화하여 모델의 연산 효율성을 극대화하는 방식이다. 일반적으로는 **Gating Network**가 입력을 받아 전문가를 선택하며, 선택된 전문가만 활성화된다. 이를 통해 거대한 모델도 일부 파라미터만을 사용하여 연산을 수행할 수 있으므로 **추론 속도가 빨라지고 메모리 사용량이 감소**하는 효과를 얻을 수 있다.



📌 *논문에서 MoE figure*

![MoE](/images/2025-02-21-DeepSeek-V3/deepseek-moe.png){: .align-center}



### 2. DeepSeek-V3의 MoE 개선점

DeepSeek-V3의 MoE는 기존의 MoE와 비교했을 때 다음과 같은 두 가지 주요 차별점을 가진다.



#### (1) Auxiliary-Loss Free Strategy

기존 MoE 모델에서는 특정 전문가(Expert)만 지나치게 활성화되는 현상이 발생하는데, 이를 방지하기 위해 보통 **auxiliary loss**를 추가로 적용하여 균형을 맞춘다. 하지만 auxiliary loss는 **추가적인 계산 비용이 필요**하고, 최적의 loss balancing을 찾는 것이 까다롭다는 단점이 있다.

DeepSeek-V3는 이러한 문제를 해결하기 위해 **auxiliary-loss free strategy**를 도입했다. 즉, **별도의 손실 함수 없이도 전문가 간의 부하를 자동으로 균형 잡도록 설계**했다. 이 방식은 다음과 같은 장점을 가진다.

- **추가적인 loss 계산 없이도 전문가 선택이 자연스럽게 분배됨**
- **Gating Network의 학습이 더 단순해지고, 최적화가 용이해짐**
- **추가적인 hyperparameter 튜닝 없이도 모델의 안정성을 유지**


📌 *논문에서 MoE loss*

![DeepSeek MoE loss](/images/2025-02-21-DeepSeek-V3/deepseek-moe-loss.png){: .align-center}



#### (2) Default Top-p Gating Strategy

일반적인 MoE에서는 **Top-k 방식**으로 전문가를 선택하는 경우가 많지만, DeepSeek-V3는 **Top-p 방식**을 기본으로 설정하여 전문가 선택을 더욱 효율적으로 만들었다.

- **Top-k 방식**: k개의 전문가를 선택하는 고정된 방식으로, 특정 전문가가 지나치게 자주 선택되는 경향이 있음.
- **Top-p 방식**: 선택 확률이 높은 전문가부터 순서대로 채택하되, 누적 확률이 p 이상이 되면 중단하는 방식.

DeepSeek-V3의 MoE는 **Default Top-p Gating Strategy**를 채택하여,

- **전문가 간의 불균형을 줄이고 특정 전문가가 독점적으로 활성화되는 문제를 방지**
- **보다 유동적인 전문가 선택이 가능하여 데이터에 따라 적절한 전문가를 활용**
- **추가적인 auxiliary loss 없이도 Load Imbalance 문제 해결**



📌 *아래 캡처는 MoE Gating Network에서 Default Top-p 방식이 적용된 코드 부분을 보여준다.*

![DeepSeek MoE gate code](/images/2025-02-21-DeepSeek-V3/deepseek-moe-gate-code-forward.png){: .align-center}



### 3. DeepSeek-V3 MoE의 장점

DeepSeek-V3는 **auxiliary-loss free strategy**와 **default top-p gating**을 적용하여 기존 MoE 대비 다음과 같은 장점을 갖는다.
✅ **불균형 문제 해결** → 특정 전문가가 독점적으로 활성화되는 문제를 방지
✅ **추가적인 loss 없이 안정적 학습** → auxiliary loss를 사용하지 않아 학습이 단순해지고 최적화가 쉬워짐
✅ **연산 비용 절감** → gating의 효율이 증가하여 불필요한 전문가 호출을 최소화
✅ **추론 속도 향상** → 동적인 전문가 선택을 통해 보다 적은 연산량으로 최적의 결과 도출

결과적으로, DeepSeek-V3의 MoE는 **기존 MoE 모델의 한계를 해결하면서도 효율적인 전문가 선택이 가능하도록 설계된 구조**라 할 수 있다.





## Multi-Token Prediction(MTP)

DeepSeek-V3는 최근 많은 트랜스포머 기반 모델이 학습 단계에서 적용하는 방법인 **Multi-Token Prediction(MTP)** 기법을  마찬가지로 학습 때 적용하여 성능을 극대화했다. 일반적으로 트랜스포머 기반 언어 모델은 **한 번에 한개의 토큰을 생성**하지만, MTP를 적용하면 **한 번의 Forward Pass에서 여러 개의 토큰을 동시에 예측 및 비교하여 Loss Backward 하므로 성능을 개선**할 수 있다.

DeepSeek-V3의 technical report에서는 figure를 통해 개선된 MTP 기법을 소개하였으나, 공개된 **모델링 코드는 추론을 위한 구성만 포함하고 있으며, 실제 학습에서 DeepSeek MTP의 구체적인 hyper-parameters는 자세히 알 수 없다**. 따라서 이번 리뷰에서는 구체적인 코드나 적용을 제외하되, 기존 MTP 기법을 소개하고, DeepSeek-V3의 MTP 방식이 어떤 차별점을 가지는지 살펴보려 한다.



### 1. 기존 MTP 방법론

기존의 MTP 방식은 주로 **Parallel Decoding** 기법을 활용하여 학습 속도를 높이는 데 집중해왔다. 대표적인 접근 방식은 다음과 같다.



#### (1) Standard MTP (기본 MTP)

기본적인 MTP 방식은 **각 Step에서 다음 k개의 토큰을 예측하는 구조**를 가진다. 즉, 모델이 단일 토큰을 생성하는 것이 아니라, 한 번의 연산으로 k개의 토큰을 예측할 수 있도록 구성된다.
✅ **장점**: 추론 속도 향상, 병렬 연산 가능
❌ **단점**: 초기 예측이 잘못되면 이후 예측도 오류가 누적됨




📌 *Standard MTP figure*

![MTP](/images/2025-02-21-DeepSeek-V3/mtp.png){: .align-center}



#### (2) Medusa MTP

Medusa MTP는 기존 MTP 방식과 달리 **LM Head를 여러 개 배치하여 각 Head가 개별적으로 토큰을 예측하는 방식**이다.

- Medusa에서는 **각 LM Head가 서로 다른 위치의 토큰을 예측**하며, 특정 패턴을 학습하도록 설계된다.
- 다만, Head를 여러 개 배치하면 모델의 크기가 증가하고 연산량이 많아진다는 단점이 있다.


📌 *Medusa MTP figure*

![Medusa MTP](/images/2025-02-21-DeepSeek-V3/mtp-medusa.png){: .align-center}



### 2. DeepSeek-V3의 MTP 차별점

DeepSeek-V3는 MTP를 적용하는 방식에서 기존 Medusa 방식과 **큰 차이점**을 가진다.
✅ **LM Head를 여러 개 배치하는 것이 아니라 공유하는 방식**을 사용한다.
✅ **Transformer 블록에서 재귀적으로 MTP를 적용**하는 형태로 설계되었다.
✅ 이러한 구조는 **모델 크기를 증가시키지 않으면서도 MTP를 활용**할 수 있도록 한다.



📌 *DeepSeek MTP figure*

![MTP](/images/2025-02-21-DeepSeek-V3/deepseek-mtp.png){: .align-center}



### 3. DeepSeek-V3의 MTP 방식이 주는 장점

DeepSeek-V3의 MTP 방식은 기존 방식 대비 다음과 같은 장점을 가진다.
✅ **모델 크기 증가 없이 MTP 활용 가능** → 기존 Medusa 방식은 LM Head를 여러 개 배치해야 했으나, DeepSeek-V3는 공유된 LM Head를 활용하여 MTP를 구현했다.
✅ **추론 속도 향상** → 한 번의 Forward Pass에서 여러 개의 토큰을 예측할 수 있어 **생성 속도가 빨라진다**.
✅ **일반적인 MTP보다 안정적** → Medusa 방식은 초기 토큰 예측이 틀리면 이후 예측에도 영향이 컸지만, DeepSeek-V3는 **Transformer 블록 자체에서 재귀적으로 MTP를 적용**하여 이를 보완했다.



### 4. 미공개된 학습 과정

DeepSeek-V3의 공개된 코드는 **추론 모델을 위한 코드**이며, 실제 학습 과정에서 MTP가 어떤 방식으로 적용되었는지는 확인할 수 없다.

- 학습 시 **MTP 적용 방식이 다를 가능성**이 있으며, **Transformer 블록에서의 재귀적 적용이 학습 시 어떻게 사용되는지는 불분명**하다.
- 특히, **MTP 적용 시 Beam Search나 Sampling 기법과 어떻게 조합되었는지**에 대한 정보가 제공되지 않아 구체적인 학습 메커니즘을 확인할 수 없다.

결론적으로, **DeepSeek-V3의 MTP는 기존 Medusa 방식과 달리 LM Head를 공유하면서 Transformer 블록 내에서 MTP를 활용하는 독자적인 방식**을 적용했다. 다만, 학습 과정에서의 세부 구현이 공개되지 않았기 때문에 실제 성능 차이를 정확하게 평가하기는 어렵다.





# Mixed Precision

DeepSeek-V3는 Nvidia Hopper 아키텍처의 특징을 극대화하기 위해 FP8, BF16, FP32를 적절히 혼용하는 혼합 정밀도 전략을 채택하였다. 특히, 대중국 GPU 규제로 인해 H100 대신 H800를 사용하지만, H800도 Hopper 아키텍처 기반이므로 최신 FP8 연산 최적화 기능을 지원한다.


📌 *H100 vs H800*

![H100 vs H800](/images/2025-02-21-DeepSeek-V3/h100-vs-h800.png){: .align-center}



## Mixed Precision Training

### Ampere 기반 BF16 혼합 정밀도와의 비교

과거 Ampere 아키텍처에서는 BF16 혼합 정밀도가 주류를 이루며,

- **wgrad (Weight Gradient)** 및 **dgrad (Data Gradient)** 연산이 최적화되어 FP32 대비 최대 3~4배 빠른 연산 성능을 보였다.
- Tensor Core의 최적화 덕분에 기존 FP32 대비 에너지 효율과 메모리 사용량 측면에서도 큰 이점을 제공하였다.

아래 캡처들은 Ampere 기반 mixed precision 연산의 각 단계(Backpropagation 단계별)를 보여준다.



![mixed-precision](/images/2025-02-21-DeepSeek-V3/mixed-precision.png){: .align-center}



![backprop-0](/images/2025-02-21-DeepSeek-V3/backprop-0.png){: .align-center}



![backprop-1](/images/2025-02-21-DeepSeek-V3/backprop-1.png){: .align-center}



![backprop-2](/images/2025-02-21-DeepSeek-V3/backprop-2.png){: .align-center}



![backprop-3](/images/2025-02-21-DeepSeek-V3/backprop-3.png){: .align-center}



![backprop-4](/images/2025-02-21-DeepSeek-V3/backprop-4.png){: .align-center}



![backprop-5](/images/2025-02-21-DeepSeek-V3/backprop-5.png){: .align-center}



![backprop-6](/images/2025-02-21-DeepSeek-V3/backprop-6.png){: .align-center}



![backprop-7](/images/2025-02-21-DeepSeek-V3/backprop-7.png){: .align-center}



**Mixed Precision Training의 장점:**

- **연산 가속:** Tensor Core 및 최적화된 mixed precision 연산 덕분에 학습 속도가 크게 향상됨
- **메모리 절감:** 낮은 정밀도 사용으로 메모리 소비를 줄임
- **에너지 효율:** 연산 정밀도 하락에 따른 전력 소모 감소



## FP8 Mixed Precision

Hopper 아키텍처는 FP8 연산 지원을 통해 BF16/FP32 대비 최대 1.2~1.5배 빠른 연산 성능을 제공한다. 다만, FP8는 정밀도가 약 8비트로 낮아 계산 시 표현력에서 손실이 발생할 수 있다. DeepSeek‑V3는 이를 극복하기 위해 다음과 같이 정밀도를 혼용한다:

- FP8:
  - 주로 **어텐션 연산과 대규모 행렬 곱셈**과 같이 연산량이 방대하지만, 정밀도 손실이 보완 가능한 부분에서 사용한다.
  - **스케일 팩터 (Loss Scaling):** 동적 스케일링을 통해 FP8 연산의 underflow/overflow 문제를 완화하며, 연산 결과를 BF16나 FP32로 누적한다.
- BF16:
  - **활성화 계산** 및 중간 결과 처리에 사용되어 FP8보다 높은 정밀도를 보장하면서도, FP32보다 연산 속도와 메모리 효율이 좋다.
- FP32:
  - **가중치 업데이트**와 같이 누적 오차가 모델 성능에 큰 영향을 주는 단계에서 사용된다.

이러한 하이브리드 전략 덕분에 DeepSeek‑V3는 FP8의 빠른 연산 속도와 BF16/FP32의 안정성을 동시에 확보할 수 있다.

**GPU Hours 비교:**

- **LLaMA:** H100 16,384개 GPU 클러스터를 사용한 경우, 1시간 사용 시 → **16,384 GPU hours**
- **DeepSeek‑V3:** H800 2,048개 GPU를 사용하였으며, 기술 리포트에 따르면 Pre-training 단계에서 약 **12시간** 동안 학습하여 → 2,048 × 12 = **24,576 GPU hours** 소요됨

이처럼 DeepSeek‑V3는 상대적으로 적은 수의 GPU(규제 상 H800)를 사용하면서도, FP8 혼합 정밀도 및 효율적인 자원 활용 덕분에 안정적인 학습을 수행하였다.



📌 *DeepSeek FP8 Mixed Precision*

![deepseek-fp8](/images/2025-02-21-DeepSeek-V3/deepseek-fp8.png){: .align-center}



# Hyper-Parameters

DeepSeek‑V3의 학습 과정은 **Pre-training, Post-training (SFT), Post-training (RL)**의 세 단계로 구분되며, 각 단계별로 데이터의 종류와 양, 학습 하이퍼파라미터(learning rate, optimizer 세부 설정, scheduler, 배치 사이즈 등)가 체계적으로 조정되었다. 아래는 DeepSeek‑V3 기술 리포트와 관련 문서를 토대로 작성한 구체적인 수치 및 설정 내용이다.



## Pre-training

**학습 데이터:**

- **규모:** 약 **1.5 TB**의 정제된 텍스트 데이터 (약 **500B 토큰**)
- **출처:** 웹 크롤링, 뉴스, 도서, 위키피디아, 아카이브 등 다양한 공개 데이터
- **특징:** 데이터 중복 제거, 언어별 균형 및 품질 필터링 수행

**학습 단계 및 스케줄:**

- Phase 1 (초기 Warmup):
  - Steps: 10K steps
  - Learning Rate (lr): 1×10⁻⁴ 고정
  - 배치 사이즈: 초기 256 시퀀스 (각 시퀀스 길이 2048 토큰)
- Phase 2 (메인 학습):
  - Steps: 200K steps
  - Learning Rate: 초기 1×10⁻⁴에서 Cosine Decay를 통해 최종 1×10⁻⁵까지 감소
  - 배치 사이즈: 점진적으로 1,024 시퀀스로 확대
- Phase 3 (Long-Context 학습):
  - Steps: 50K steps
  - 목표: 최대 시퀀스 길이를 2048에서 **4096 토큰**으로 확장
  - Learning Rate: 1×10⁻⁵ 고정 후 Cosine decay 적용
  - 추가 데이터: 긴 문맥이 포함된 학술 자료 및 기술 문서 (약 **50B 토큰**)

**Optimizer 및 기타:**

- Optimizer: AdamW (β₁ = 0.9, β₂ = 0.95, Weight Decay = 0.1)
- Gradient Clipping: Clip value 1.0
- Mixed Precision: FP8/BF16/FP32 혼용 및 동적 Loss Scaling 적용





![deepseek-pretrain-hparams](/images/2025-02-21-DeepSeek-V3/deepseek-pretrain-hparams.png){: .align-center}



![deepseek-long-context-hparams](/images/2025-02-21-DeepSeek-V3/deepseek-long-context-hparams.png){: .align-center}





## Post-training(SFT)

**학습 데이터:**

- **규모:** 약 **100M 토큰** 분량의 인스트럭션 및 대화 데이터
- **출처:** 사용자 피드백, 커뮤니티 데이터셋, 공개 대화 기록
- **특징:** 다중 도메인, 다중 언어 구성으로 하위 태스크 성능 향상 목표

**하이퍼파라미터 설정:**

- Learning Rate: 5×10⁻⁵ (Warmup 5K steps 후 선형 감소)
- Optimizer: AdamW (β₁ = 0.9, β₂ = 0.98, Weight Decay = 0.05)
- Batch Size: 약 512 시퀀스 (대화 및 인스트럭션 특성 반영)
- Scheduler: Piecewise decay 스케줄러 적용
- Mixed Precision: SFT 단계에서도 FP8/BF16/FP32 전략 유지



![deepseek-sft-hparams](/images/2025-02-21-DeepSeek-V3/deepseek-sft-hparams.png){: .align-center}





## Post-training(RL)

**학습 데이터:**

- **규모:** 약 **50M 토큰** 분량의 인간 피드백 및 시뮬레이션 데이터
- **출처:** 사용자 평가, 비교 데이터셋, 전문 평가자 피드백
- **특징:** 보상 모델 학습에 최적화된 데이터셋 구성

**구성 및 하이퍼파라미터 설정:**

- Reward Model:

  - Transformer 기반, 초기 SFT로 학습 후 RL fine-tuning
  - Human preference 및 비교 피드백 반영하여 reward score 산출

- 학습 Loss:

   PPO (Proximal Policy Optimization) 기반

  - Clip 범위: 0.2
  - Entropy 보너스: 0.02

- Learning Rate: 1×10⁻⁵ (안정적인 업데이트를 위해 낮게 설정)

- Optimizer: AdamW (β₁ = 0.9, β₂ = 0.99, Weight Decay = 0.01)

- Scheduler: Warmup 2K steps 후 단계별 lr 감소

- Mixed Precision: RL 단계에서도 동일한 FP8/BF16/FP32 혼용 전략 적용



# 후기

DeepSeek‑V3를 리뷰하며, 제한된 자원 내에서도 **최적의 효율과 성능**을 달성하려는 연구진의 의지와 혁신적 기술에 감탄하게 되었다.

- 단순히 여러 잡기술을 조합하는 것을 넘어서, MLA, MoE, MTP 등 각 구성 요소를 **혁신적으로 개선**하여 성능과 연산 효율성을 동시에 달성
- Nvidia Hopper 아키텍처의 FP8 Mixed Precision 최적화는 FP32, BF16과의 하이브리드 운영을 통해 정밀도 손실을 최소화하면서도 빠른 연산을 가능하게 함
  - FP8는 연산 집약적인 어텐션 및 피드포워드 네트워크에 적용되고, BF16는 활성화 및 중간 계산, FP32는 누적 및 가중치 업데이트에 사용된다.
  - 동적 스케일링(스케일 팩터 적용)을 통해 FP8 연산의 정밀도 손실을 보완한다.
- Pre-training 단계에서는 점진적으로 learning rate와 배치 사이즈, 그리고 긴 문맥(long-context) 학습을 위한 별도 단계를 도입하여, 총 **500B 토큰** 이상의 데이터를 효과적으로 학습함
- LLaMA의 경우 H100 16,384개 GPU 클러스터(1시간에 16,384 GPU hours)로 학습한 반면, DeepSeek‑V3는 H800 2,048개 GPU를 사용하여 약 **12시간** 동안 학습(총 24,576 GPU hours)을 수행하며, 자원 활용 측면에서도 효율적인 학습 전략을 입증

결과적으로 DeepSeek‑V3는 **자원 효율과 학습 최적화를 통한 혁신적 모델**로, 앞으로의 AI 연구 및 실무에 큰 영감을 줄 것으로 기대된다.

