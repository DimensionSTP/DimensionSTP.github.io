---
layout: single
title: "Wan2.1 Review"
categories: Study-concept
tag: [Wan2.1, Diffusion , Video Diffusion, Text to Video, Image to Video, DiT, Alibaba]
toc: true
author_profile: false
sidebar:
    nav: "docs"
search: true
typora-root-url: ../
---





# 0. Introduction

👉🏻[Wan2.1 GitHub link](https://github.com/Wan-Video/Wan2.1)

👉🏻[Wan2.1 model link](https://huggingface.co/Wan-AI/Wan2.1-T2V-14B)

👉🏻[Wan2.1 technical report link](https://arxiv.org/pdf/2503.20314)

👉🏻[Wan2.1 blog link](https://wan.video/welcome?spm=a2ty_o02.30011076.0.0.6c9ee41eCcluqg)

👉🏻[Wan2.1 playground link](https://wan.video/)



*“텍스트 한 줄, 한 장의 이미지만으로도 영화 같은 5 초짜리 영상을 만드는 시대.”* — 알리바바 **Tongyi Lab**의 **Wan 2.1**은 2025년 2월 공개된 최초의 **완전 오픈소스 SOTA 비디오 생성 모델**이다. 텍스트‑투‑비디오(T2V)·이미지‑투‑비디오(I2V)는 물론, 비디오 편집과 오디오 합성까지 지원한다. 본 글에서는 Wan 2.1의 **아키텍처, 학습 방법, 데이터셋, 하이퍼파라미터**를 논문 수준으로 해부하고, **Video Diffusion**의 미래를 조망한다.



### 주요 특징

Wan 2.1은 공개 이후 **“오픈소스 SORA”**라 불리며 연구·창작 커뮤니티의 뜨거운 관심을 받았다. 핵심 키워드는 **3D Causal VAE + Diffusion Transformer + Flow Matching + 초거대 데이터**. 특히 다국어 T2V에서 **영상 내부 텍스트**까지 정확히 합성 가능한 첫 모델이라는 점이 인상적이다.

- **3D Causal VAE —** 시간 인과성을 보존하면서 64× 압축.
- **Flow Matching —** 노이즈 제거를 연속 확률 흐름으로 표현해 안정·가속.
- **15 B Video + 10 B Image —** 시각‑텍스트 정합 & 하드 케이스 학습.
- **6‑단계 Curriculum —** 해상도·복잡도를 점진적으로 상승.
- **VBench 0.724 —** Sora(0.700), Gen‑2(0.672)를 능가.



# 1. Wan2.1 Model Architecture



## 1-1. Overview

| 항목                  | 14B (720p)              | 14B (480p) | 1.3B (480p) |
| --------------------- | ----------------------- | ---------- | ----------- |
| **파라미터 수**       | ≈ 14.0 B                | ≈ 14.0 B   | ≈ 1.3 B     |
| **해상도**            | 1280×720                | 960×540    | 960×540     |
| **프레임**            | 80 (5 s@16 fps)         | 80         | 80          |
| **DiT 레이어/임베딩** | 40 / 5120               | 40 / 5120  | 30 / 1536   |
| **VAE 다운샘플**      | 8× spatial, 4× temporal | 동일       | 동일        |
| **데이터**            | 15 B video + 10 B image | 동일       | 동일        |
| **라이선스**          | Apache 2.0              | Apache 2.0 | Apache 2.0  |



📌 *Wan2.1 Architecture*

![Wan2.1-Architecture0](/images/2025-05-07-Wan2dot1/Wan2dot1-Architecture0.jpg){: .align-center}



📌 *Wan2.1 Architecture*

![Wan2.1-Architecture1](/images/2025-05-07-Wan2dot1/Wan2dot1-Architecture1.png){: .align-center}



## 1-2. Details

### Wan‑VAE 

#### 설계 목표 {#wan-vae-설계-목표}

1. **시간 인과성** — 미래 프레임 참조 없이 현재 프레임 인코딩.
2. **메모리 효율** — 긴 영상도 노트북 GPU에서 스트리밍 가능.
3. **재현 품질** — PSNR · SSIM ≥ 동급 2D VAE + 1 dB 이상.

#### 인코더 구조 {#wan-vae-인코더-구조}

- 입력 : RGB $T=80$ frames, $H×W∈{720,540}$.
- **Conv3D‑Causal × 5 블록** → 시간 축 causal padding.
- **Downsample (2×) × 3** → 최종 8× spatial 축소, 4× temporal 축소.
- 출력 : `latent_seq ∈ R^{(1+T/4) × (H/8) × (W/8) × C}`.

#### 디코더 구조 {#wan-vae-디코더-구조}

- **ConvTranspose3D 블록** × 대칭으로 구성.
- Skip connection : 인코더 각 단계 feature → 대응 디코더 단계.
- 마지막 층 : `tanh` → [-1,1] 픽셀 범위.

#### 히스토리 캐시 & 청크 스트리밍 {#wan-vae-히스토리-캐시}

- **청크 크기 4f** — 인코딩 시 과거 latents 캐시에 저장.
- **Decoder**는 동일 청크 단위로 순차 복원 → GPU 2 GB 메모리 세이브.



### Diffusion Transformer (DiT) 

#### 블록 구성 & 파라미터 {#dit-블록-구성}

- **Layer = 40 (14B) / 30 (1.3B)**.
- **Self‑Attention** → 토큰수 ≈ (21 latents) × H/8 × W/8 ≈ 13 k.
- **FFN hidden** = 8 × d_model.
- **GEGLU** 활성화.

#### 시간 임베딩 ⚡ Adaptive LN {#dit-시간-임베딩}

- `t_embed = MLP(sinusoid(t))` (128‑d).
- 각 블록 AdaLN: `y = (1 + s) * (x / √Var + ε) + b`, where `(s,b)` ← Linear(`t_embed`).
- **공유 MLP** → 파라미터 효율, 단계별 일관성.

#### 크로스 어텐션 ↔ 텍스트·이미지 조건 {#dit-크로스-어텐션}

- Query = video latents, Key/Value = `concat(text_emb, img_emb)`.
- **Class‑free** 조건 블랭크 → CFG 때 사용.

#### Flow Matching 객관식 & 학습 안정화 {#dit-flow-matching}

- 타깃 : $\mathbf{F}(x,t) = -σ(t)∇_x log p_t(x)$.
- 모델 $\hat F_θ$ 로 **ODE integrator**(DDIM) 해석, 스텝 수 ↓.



### UMT5 텍스트 인코더 

#### 다국어 사전학습 {#umt5-다국어}

- 2 T 토큰, 100+ 언어.
- **자체 BPE 100k** → 중국어·영어 토큰 효율 ↑.

#### 텍스트‑비주얼 브릿지 {#umt5-브릿지}

- Text [CLS] embedding → DiT global prompt.
- Token‑wise cross‑attn → fine‑grained alignment.



### I2V 이미지 조건 분기 

#### CLIP Vision 인코더 {#i2v-clip}

- ViT‑H @ 14 patch, 1280‑d.
- Linear → d_model dim, LayerNorm.

#### 첫 프레임 잠재 삽입 & 정합 유지 {#i2v-첫-프레임}

- `latent_seq[0] = VAE(img)` 대체.
- **Noise schedule shift** → 첫 프레임 복사 but 나머지프레임 노이즈.

### 부가 모듈 : 비디오‑투‑오디오 {#부가-모듈}

- Auto‑tag video → Text2Audio LLM (Qwen‑Audio) → WAV.




# 2. Train Methods



## 2-1. 초거대 데이터 파이프라인

#### 1차 — 기본 필터링 {#데이터-1차-필터}

- **장면 끊김**, **블랙 바**, **워터마크** 검출 모델.
- FPS < 8 or > 60 → 제거.

#### 2차 — 시맨틱 군집 & 품질 스코어 {#데이터-2차-시맨틱}

- ViT ‑ B/32 feature → K‑means 100 클러스터.
- 각 클러스터 500 샘플 인수 작업 → 작은 평가 모델.

#### 3차 — 시각적 텍스트 강화 {#데이터-3차-텍스트}

- 합성: 폰트 8k × 배경 3k.
- 실사: pan‑OCR → text box IOU > 0.7 선택.

#### 4차 — 고품질 후처리 세트 {#데이터-4차-후처리}

- PQV (Photo Quality Value) 상위 20 %.
- 동영상 : Motion Complexity ↑, Scene Diversity ↑.



## 2-2. 6단계 커리큘럼 학습

| 단계 | 해상도   | 길이 | 배치  | LR   | 목적                    |
| ---- | -------- | ---- | ----- | ---- | ----------------------- |
| 1    | 256² img | —    | 32 k  | 1e‑3 | 텍스트‑시각 매핑 초기화 |
| 2    | 192²     | 5 s  | 8 k   | 6e‑4 | 움직임 패턴 습득        |
| 3    | 480²     | 5 s  | 4 k   | 4e‑4 | 디테일 ↑                |
| 4    | 720²     | 5 s  | 2 k   | 3e‑4 | 고해상도 적응           |
| 5    | 480²     | 5 s  | 1 k   | 1e‑4 | 고품질 FT(480p)         |
| 6    | 720²     | 5 s  | 0.5 k | 8e‑5 | 고품질 FT(720p)         |



## 2-3. 손실 함수 & Guidance

#### Flow Matching Loss {#flow-matching-loss}

$\mathcal L_{FM}=\mathbb E\Big[\big|\hat F_θ(x,t)−F(x,t)\big|_2^2\Big]$

#### CFG & CFG‑Zero {#cfg}

- Scale 3.5 ~ 7.0.
- CFG‑Zero: 무조건 분기 replace → “dropout mask” α = 0.1.



## 2-4. 분산 & 메모리 최적화

#### RingAttention · Ulysses 시퀀스 병렬 {#분산-어텐션}

- 토큰 길이 ≈ 13 k → GPU 32 GB 초과 ⚠️.
- Query × Key 분할 → all‑reduce cost ↓ 45 %.

#### Activate Offload & Checkpoint {#메모리-옵션}

- GPU↔CPU  pipelined swap, step latency +2 % … VRAM ‑30 %.

#### 지능형 노드 스케줄러 {#노드-스케줄러}

- Node health ping → slow node 10× 미만시 자동 축출.



# 3. Hyper-Parameters



## 3-1. Train

| 구분                | 값                                      |
| ------------------- | --------------------------------------- |
| **Optimizer**       | *AdamW* (β₁ = 0.9, β₂ = 0.95, ε = 1e‑8) |
| **LR schedule**     | Cosine decay w/ 10 % warm‑up            |
| **Base LR**         | 4e‑4 (14B) / 6e‑4 (1.3B)                |
| **Total batch**     | 4 096 (14B) / 8 192 (1.3B)              |
| **Grad accum**      | 8 steps                                 |
| **EMA**             | 0.9995                                  |
| **Diffusion steps** | 50 (추론) / 1 000 (학습)                |
| **CFG scale**       | 3.5 – 7.0                               |
| **Dropout**         | 0.1 (attn / ffn)                        |



## 3-2. Inference

| 항목           | 기본 | 비고                |
| -------------- | ---- | ------------------- |
| Sampling steps | 50   | DDIM solver         |
| CFG Scale      | 5.0  | ≥7 : overshoot risk |
| Seed           | 42   | 재현성              |
| Output FPS     | 16   | 변경 가능           |



# 4. Evaluation & Comparison



### VBench 종합 점수 {#vbench}

- Wan 2.1‑14B **0.724** > Sora 0.700 > Gen‑2 0.672 > Pika 0.612.



### 항목별 리더보드

| 카테고리      | 1위 모델 | Wan 점수 |
| ------------- | -------- | -------- |
| 인물 동일성   | Wan 2.1  | 0.81     |
| 물리 정합성   | Sora     | 0.78     |
| 카메라워크    | Wan 2.1  | 0.83     |
| 플리커 최소화 | Gen‑2    | 0.77     |
| OCR 정확도    | Wan 2.1  | 0.85     |

### 어블레이션 스터디

- **Flow Matching ↘** DDPM 교체 → FVD +7 ↗︎.
- **시간 임베딩 공유 MLP 제거** → CLIP‑SIM ‑0.3.



### 제한점 & 오류 사례

1. **긴 장면 전환** 시 동일 인물 얼굴 drift.
2. **고주파 텍스처**(잔디, 물결)에서 moire pattern.
3. 10 s 이상 시퀀스 → 플리커 증가, 메모리 폭증.



# 5. Summary & Insights



#### Token Compression & 3D VQVAE {#미래-토큰}

- 프레임당 토큰 수 ↓ ~16× → 어텐션 O(n²) 부담 완화.

#### Sparse Global Attention {#미래-스파스}

- Per‑frame block sparse + periodic global token → 장면 일관성 ↑.

#### Cascade Upsampling {#미래-캐스케이드}

- MS‑VideoFusion 구조: 128² → 256² → 512² → 1080².

#### 3D Coherence Loss {#미래-coherence}

- CLIP‑VIT + T‑ViT dual loss로 플리커 FVD ‑15 달성 사례.

#### 멀티모달 융합 → Scene Generation {#미래-멀티모달}

- Text + Layout + Audio + Physics Prompt → 인터랙티브 씬.



Wan 2.1은 **오픈소스 비디오 생성의 체급을 한 단계 올린 모델**이다. 3D Causal VAE와 Flow Matching Transformer라는 독창적 조합, 초거대 데이터 커리큘럼, 효율적인 분산 학습이 맞물려 나타난 결과이다. 앞으로 토큰 압축·스파스 어텐션 기법, 멀티스테이지 업샘플러가 핵심 과제가 될 것이며, 비디오 생성은 조만간 **“텍스트 → 영화”**로 진화할 것이다.
