---
layout: single
title:  "DeepSeek-R1 Review"
categories: Study-concept
tag: [DeepSeek, DeepSeek-R1, Reasoning, Reasoning Model]
toc: true
author_profile: false
sidebar:
    nav: "docs"
search: true
typora-root-url: ../
---





#  DeepSeek-R1 Review

👉🏻[논문 링크](https://arxiv.org/pdf/2501.12948)

👉🏻[비주얼 가이드](https://tulip-phalange-a1e.notion.site/DeepSeek-R1-189c32470be2801c94b6e5648735447d)

DeepSeek‑R1는 최근 대규모 언어모델(LLM)의 추론 능력을 강화하기 위해 순수 강화학습(RL)을 활용한 첫 번째 Reasoning 모델로 주목받는다. DeepSeek‑R1‑Zero라는 순수 RL 기반 모델을 선보인 후, 읽기 쉬운 출력과 언어 일관성 문제를 보완하기 위해 소량의 “cold‑start” 데이터와 다단계 훈련 파이프라인을 추가한 DeepSeek‑R1이 탄생했다. 이번 리뷰에서는 DeepSeek‑R1의 핵심 아이디어, RL 기반 추론 강화 과정, 그리고 소형 모델로의 증류 전략 등을 중점적으로 살펴본다.



# Key Innovations & Pipeline Overview

DeepSeek‑R1의 주요 기여는 두 가지로 나눌 수 있다.



### 1. Pure RL 기반 Reasoning (DeepSeek‑R1‑Zero)

- 순수 강화학습으로 추론 능력 증대:
  - 초기 DeepSeek‑R1‑Zero는 SFT 없이, 오직 RL만으로 모델이 체인‑오브‑씽킹(CoT)을 자발적으로 확장하는 과정을 학습했다.
  - RL 과정 중 AIME 2024의 pass@1 점수가 15.6%에서 71.0%로 크게 향상되었으며, 다수결(majority voting)을 적용하면 86.7%까지 상승하여 OpenAI‑o1‑0912와 견줄 만한 성능을 달성했다.
- 한계:
  - 하지만, 출력의 가독성이 떨어지고 언어 혼합 현상이 발생하는 문제를 보였다.



### 2. Cold‑Start 데이터와 다단계 훈련을 통한 DeepSeek‑R1

- Cold‑Start 데이터 활용:
  - 수천 건의 고품질, 장기 CoT 데이터를 수집하여 DeepSeek‑V3‑Base 모델을 초기 RL 액터로 미세조정함으로써, RL 초기의 불안정성을 완화하고 읽기 좋은 출력을 확보하였다.
- 다단계 훈련 파이프라인:
  - 초기 Cold‑Start 미세조정 후, 기존 DeepSeek‑R1‑Zero와 유사한 reasoning‑oriented RL을 적용.
  - RL 수렴 후, Rejection Sampling을 통해 SFT 데이터(문제, 원본 응답, 시스템 프롬프트를 포함한 양식)를 추가하고, 다시 RL 단계를 거쳐 최종 모델 DeepSeek‑R1을 완성하였다.
- 증류 전략:
  - DeepSeek‑R1의 추론 패턴을 소형 dense 모델(1.5B, 7B, 8B, 14B, 32B, 70B)로 증류해, 소형 모델에서도 강력한 Reasoning 능력을 구현하는 데 성공하였다.



# Approach & Methodology

DeepSeek‑R1의 학습 파이프라인는 크게 세 단계로 구성된다.



## DeepSeek‑R1‑Zero: 순수 RL 접근

- RL 알고리즘:
  - Group Relative Policy Optimization (GRPO)을 사용하여, critic 모델 없이 그룹 내 보상 통계를 기반으로 정책 업데이트를 수행한다.
- Reward Modeling:
  - **Accuracy Rewards:** 문제의 정답 형식(예: 수학 문제의 경우 정해진 상자 형식) 등 명확한 규칙을 통한 보상.
  - **Format Rewards:** 출력 내 ‘<think> … </think>’ 태그를 활용하여, 모델이 추론 과정을 명시하도록 유도.
- Self-Evolution:
  - RL 과정을 통해 모델은 스스로 ‘생각하는 시간’을 늘리며 복잡한 문제에 대한 다양한 해결 전략을 자발적으로 학습한다.
  - 중간 “aha moment”를 통해, 초기 접근 방식을 재평가하며 더 효과적인 문제 해결 방법을 발견한다.



## DeepSeek‑R1: Cold‑Start와 다단계 RL

- Cold‑Start 데이터 수집:
  - 몇 천 건의 고품질 long CoT 데이터를 수집, 출력의 가독성과 언어 일관성을 확보하는 형식으로 재정의(예: |special_token|<reasoning>|special_token|<summary>).
- Reasoning‑Oriented RL:
  - Cold‑Start로 초기 미세조정된 모델에 대해 동일한 RL 기법을 적용하여, 문제 해결 능력과 언어 일관성을 동시에 강화.
  - 언어 혼합 문제를 줄이기 위해 ‘language consistency reward’를 도입.
- Rejection Sampling 및 SFT:
  - RL 수렴 후, 다양한 도메인의 데이터를 포함한 SFT 데이터셋(약 800K 샘플)을 생성하여 추가 미세조정 진행.
  - 이를 통해 모델은 단순한 수학, 코딩 문제뿐 아니라 창의적 글쓰기, 역할극, 일반 QA 등 다양한 태스크에 대응할 수 있게 된다.



## Distillation: 소형 모델로의 Reasoning 능력 증류

- 증류 전략:
  - DeepSeek‑R1을 teacher 모델로 사용하여, Qwen 및 Llama 기반의 소형 모델에 대해 SFT만 적용하여 증류.
  - 증류된 모델은 DeepSeek‑R1의 추론 패턴을 효과적으로 내재화하여, Reasoning 벤치마크(예: AIME 2024, MATH‑500, Codeforces 등)에서 경쟁력 있는 성능을 보여준다.



# Evaluation & Performance

- 논문에 따르면 DeepSeek‑R1은 다양한 Reasoning 벤치마크에서 뛰어난 성능을 보인다.

  - Mathematics:
    - AIME 2024에서는 DeepSeek‑R1이 약 79.8% Pass@1을 기록, OpenAI‑o1‑1217과 유사한 수준을 달성.
    - MATH‑500에서는 97.3%에 육박하는 성능을 보이며, 기존 모델 대비 큰 성능 향상을 입증하였다.
  - 코딩 및 기타 Reasoning 태스크:
    - Codeforces에서는 높은 백분위와 Elo rating(DeepSeek‑R1의 경우 약 2029점)을 달성, 코딩 경진대회에서도 상위권을 유지.
    - MMLU, GPQA Diamond, 그리고 일반 QA 등에서도 DeepSeek‑R1은 DeepSeek‑V3 대비 우수한 성능을 보이며, 특히 STEM 분야에서 큰 강점을 나타낸다.
  - 출력 형식 및 길이:
    - 출력의 요약 길이는 평균 689 tokens(AlpacaEval 2.0) 내지 2,218 characters로, 가독성을 높이면서도 필요한 정보를 압축하여 제공한다.

  또한, 증류된 소형 모델(DeepSeek‑R1‑Distill-Qwen 및 DeepSeek‑R1‑Distill-Llama 시리즈)은 OpenAI‑o1‑mini 및 기타 최신 모델과 비교해 경쟁력 있는 성능을 입증하며, 향후 소형 모델 개발에 큰 기여를 할 것으로 기대된다.



# 후기

DeepSeek‑R1은 순수 RL과 소량의 Cold‑Start 데이터를 활용해 LLM의 Reasoning 능력을 획기적으로 강화한 첫 모델로, 다음과 같은 점에서 주목할 만하다.

- **순수 RL의 가능성**:
  - DeepSeek‑R1‑Zero를 통해 SFT 없이도 강력한 Reasoning 능력이 자발적으로 학습될 수 있음을 입증.
- **Cold‑Start와 다단계 훈련의 효과**:
  - 초기 불안정성을 극복하고, 출력의 가독성과 언어 일관성을 확보함으로써 사용자 친화적인 모델로 발전.
- **소형 모델 증류**:
  - 대형 모델의 Reasoning 패턴을 소형 모델에 증류해, 자원 효율성과 실무 적용 가능성을 동시에 높임.

향후 DeepSeek‑R1은 다중 턴 대화, 복잡한 역할극, 함수 호출 등 실무적 응용 분야로 확장될 전망이며, 언어 혼합 문제와 같은 한계도 지속적인 연구를 통해 보완할 계획이다.

