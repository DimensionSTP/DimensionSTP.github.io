---
layout: single
title:  "DeepSeek-V3 Review"
categories: Study-concept
tag: [DeepSeek, DeepSeek-V3, MLA, MoE, MTP, Mixed-precision, Hopper architecture]
toc: true
author_profile: false
sidebar:
    nav: "docs"
search: true
typora-root-url: ../
---





#  DeepSeek-V3 Review

👉🏻[논문 링크](https://arxiv.org/abs/2412.19437 "DeepSeek V3") 

DeepSeek-V3에 대한 리뷰는 이미 많이 나왔다. 단순한 성능 비교나 벤치마크 결과를 나열하는 것은 의미가 없다고 생각한다.

대신, 이번 리뷰에서는 **최적의 성능을 끌어내는 트랜스포머 아키텍처의 설계와 효율적인 학습을 위한 혼합 정밀도 활용**에 집중하려 한다.

특히,

- 전통적인 디코더 기반 트랜스포머 모델을 어떤 기술들을 적용해서 **개선을 거듭**하여 전반적인 효율 및 성능을 향상시켰는지
- Nvidia Hopper Architecture를 최대한 활용한 **FP8 mixed precision**이 어떻게 성능은 유지하면서도 자원 및 시간을 절감시켰는지
- 기타 학습 hyper-parameters

이 세 가지 핵심 요소를 중심으로 분석해보려고 한다. DeepSeek-V3의 아키텍처와 학습 기법이 기존 모델과 어떤 차별점을 가지는지 살펴보자.



# Architectures

- DeepSeek V3의 아키텍처는 기존의 전통적인 디코더 기반 트랜스포머 모델 대비 3가지가 다름
  - MLA
    - MLA는 딥시크의 독자적 구조
  - MoE
    - MoE는 많이 존재해왓으나 딥시크만의 아이디어 첨가(auxiliary-loss free strategy, default top p)
  - MTP
    - MTP는 기존이나 메두사와 같이 LM head를 나누는게 아니라 lm head는 공유, 트랜스포머 블록에 재귀적으로 태우는 방식
    - 추론 모델링 코드에는 구현되어있지 않아서 구체적으로 알 수는 없음

시작



DeepSeek-V3의 아키텍처는 기존의 **전통적인 디코더 기반 트랜스포머 모델**과 비교했을 때 세 가지 중요한 차이점을 가진다.



1. **Multi-head Latent Attention(MLA)**
   기존의 어텐션 메커니즘은 모델 크기가 커질수록 연산량과 메모리 사용량이 증가하는 문제를 안고 있었다. DeepSeek-V3는 이를 해결하기 위해 **MLA(Multi-head Latent Attention)**라는 자체적인 어텐션 구조를 도입했다. MLA는 기존의 Multi-Head Attention(MHA)와 달리 **Q, K, V 행렬을 처음부터 LoRA(Low-Rank Adaptation) 방식으로 구성**하고, K와 V를 **하나의 projection에서 생성**하여 모델의 파라미터 효율성을 극대화했다. 또한, MLA는 기존 RoPE(Rotary Position Embedding)와 함께 **NoPE(No Position Embedding)** 방식을 혼용하여 다양한 위치 인코딩 전략을 적용했다. MLA는 GQA(Grouped Query Attention) 및 MQA(Multi-query Attention) 방식을 **혼합적으로 사용**하여, 연산 효율을 유지하면서도 높은 성능을 확보했다.

   

2. **Mixture of Experts(MoE)**
   MoE(Mixture of Experts)는 기존에도 여러 모델에서 활용된 기법이지만, DeepSeek-V3는 이를 더욱 발전시켰다. 일반적인 MoE 모델에서는 전문가 네트워크 간의 균형을 맞추기 위해 auxiliary loss를 추가적으로 사용하지만, DeepSeek-V3는 **auxiliary-loss free strategy**를 도입하여 이러한 추가적인 손실 없이도 전문가 네트워크를 효율적으로 활용할 수 있도록 했다. 또한, **기본적으로 Top-p 방식의 전문가 선택(default top-p strategy)을 채택**, 특정 전문가가 과도하게 활성화되는 문제를 방지하고 보다 균형 잡힌 학습이 이루어지도록 설계했다. 이러한 접근 방식은 모델이 더욱 안정적이고 효율적으로 학습할 수 있도록 도와준다.

   

3. **Multi-Token Prediction(MTP)**
   기존의 MTP(Multi-Token Prediction) 방식은 트랜스포머 기반 모델에서 학습 속도를 높이기 위해 자주 사용되었다. 일반적으로는 Medusa와 같은 구조에서 LM head를 여러 개로 나누어 각 head가 다른 토큰을 예측하는 방식이었지만, DeepSeek-V3는 **LM head를 공유하면서 Transformer 블록을 재귀적으로 활용하는 방식**을 채택했다. 이를 통해 파라미터 수를 증가시키지 않으면서도 한 번의 forward pass에서 여러 개의 토큰을 예측할 수 있도록 설계되었다. 다만, 현재 공개된 **DeepSeek-V3의 추론 모델링 코드에는 해당 MTP 기법이 구현되어 있지 않아** 실제 학습 과정에서 어떤 방식으로 적용되는지는 구체적으로 확인할 수 없다.

   

이러한 세 가지 핵심적인 차별점을 통해 DeepSeek-V3는 **동일한 파라미터 수에서 더 나은 성능을 제공하면서도 연산 효율성을 극대화하는 구조를 갖추었다**. 다음으로는 각 아키텍처에 대해 더욱 상세히 분석해보자.



## Multi-head Latent Attention(MLA)

DeepSeek-V3에서 가장 눈에 띄는 차별점 중 하나는 **Multi-head Latent Attention(MLA)**이다. 기존의 트랜스포머 모델에서 사용되는 **Multi-Head Attention(MHA)**는 각 헤드마다 독립적인 Key(K)와 Value(V)를 생성하는 방식이었지만, MLA는 이를 개선하여 **연산량과 메모리 사용량을 줄이면서도 성능을 유지하는 방식**을 도입했다.



### 1. MLA의 핵심 개념

일반적인 트랜스포머에서는 Q, K, V 행렬을 각 헤드마다 독립적으로 생성하지만, DeepSeek-V3의 MLA는 **LoRA(Low-Rank Adaptation) 기법을 활용하여 Q, K, V를 처음부터 경량화된 형태로 생성**한다. 특히, **K와 V를 하나의 projection에서 생성**하는 구조를 갖추고 있어, 파라미터 수를 줄이면서도 유사한 어텐션 성능을 유지할 수 있도록 설계되었다.



![MLA](/images/2025-02-19-DeepSeek-V3/deepseek-mla.png){: .align-center}



👉 **[MLA 코드 캡처 위치]**
📌 *아래 캡처는 MLA에서 LoRA를 활용하여 Q, K, V를 생성하는 코드 부분을 보여준다.*

> **코드 캡처: MLA에서 Q, K, V를 LoRA로 생성하는 부분**



### 2. RoPE vs. NoPE: MLA에서의 위치 인코딩 방식

MLA는 **RoPE(Rotary Position Embedding)와 NoPE(No Position Embedding)를 동시에 지원**하는 구조를 갖추고 있다.

- **RoPE**는 기존 Llama 시리즈에서 사용된 방식으로, 상대적 위치 정보를 어텐션 메커니즘에 자연스럽게 주입할 수 있는 강점을 가진다.

- NoPE

  는 특정한 위치 인코딩을 사용하지 않고, MLA 자체의 학습 과정에서 자연스럽게 패턴을 익히도록 하는 방식이다.

  - MLA에서는 GQA(Grouped Query Attention) 및 MQA(Multi-Query Attention)를 **동시에 활용**할 수 있도록 설계되었으며, 이때 Key 헤드 차원(`k head dim`)이 1이면 **RoPE 또는 NoPE 방식 중 하나를 선택하여 적용**한다.

👉 **[RoPE vs. NoPE 비교 피규어 위치]**
📌 *아래 그림은 MLA에서 RoPE와 NoPE를 적용하는 구조적 차이를 시각적으로 보여준다.*

> **피규어 캡처: MLA에서 RoPE와 NoPE가 적용된 구조 비교**



### 3. MLA와 기존 Llama 어텐션의 차이점

기존의 Llama 모델에서도 어텐션 최적화를 위한 여러 기법이 적용되었지만, MLA는 다음과 같은 추가적인 개선점을 포함하고 있다.

- **LoRA 기반의 경량 Q, K, V 생성**
- **K, V를 하나의 projection에서 추출하여 메모리 효율성 증가**
- **RoPE 및 NoPE 선택 가능 (GQA, MQA 동시 활용 가능)**

👉 **[기존 Llama 어텐션 vs. MLA 비교 피규어 위치]**
📌 *아래 그림은 기존 Llama 모델과 DeepSeek-V3의 MLA를 비교하여 차이점을 보여준다.*

> **피규어 캡처: Llama 어텐션 vs. MLA 비교**



### 4. MLA의 성능 및 기대 효과

이러한 설계를 통해 MLA는 기존 MHA 대비 **메모리 사용량을 줄이면서도 동일한 수준의 성능을 유지**하는 데 초점을 맞췄다. 특히, **LoRA를 활용한 Q, K, V 생성과 K, V의 단일 projection 사용으로 인해 연산량을 절감**할 수 있다는 점이 큰 장점이다.

👉 **[MLA 코드 전체 구조 캡처 위치]**
📌 *아래 캡처는 MLA 전체 구현 코드로, 기존 MHA와의 차별점을 강조한다.*

> **코드 캡처: MLA의 전체 어텐션 구현 구조**

결론적으로, MLA는 기존 어텐션 구조에서 발생하는 **메모리 및 연산량 문제를 효과적으로 개선하면서도 성능을 유지하는 독자적인 DeepSeek-V3의 핵심 아키텍처**라 할 수 있다.





## Mixture of Experts(MoE)

👉🏻[DeepSeek MoE 논문 링크](https://arxiv.org/pdf/2401.06066 "DeepSeek MoE") 

DeepSeek-V3는 기존에도 널리 사용되던 **Mixture of Experts (MoE)** 구조를 채택했지만, 단순한 MoE 모델이 아니라 **효율성과 균형을 최적화한 새로운 전략을 도입**했다. 일반적으로 MoE는 거대한 모델에서 연산량을 줄이면서도 성능을 유지할 수 있도록 하는 강력한 기법이지만, 전문가 네트워크 간의 **부하 불균형(Load Imbalance) 문제**와 **추가적인 auxiliary loss 필요성**이 단점으로 지적되어 왔다. DeepSeek-V3는 이러한 한계를 해결하기 위해 **auxiliary-loss free strategy**와 **default top-p gating** 기법을 도입했다.



### 1. MoE의 기본 개념

MoE는 여러 개의 전문가(Expert) 네트워크 중 일부만 활성화하여 모델의 연산 효율성을 극대화하는 방식이다. 일반적으로는 **Gating Network**가 입력을 받아 전문가를 선택하며, 선택된 전문가만 활성화된다. 이를 통해 거대한 모델도 일부 파라미터만을 사용하여 연산을 수행할 수 있으므로 **추론 속도가 빨라지고 메모리 사용량이 감소**하는 효과를 얻을 수 있다.



![MoE](/images/2025-02-19-DeepSeek-V3/deepseek-moe.png){: .align-center}



👉 **[MoE 피규어 위치]**
📌 *아래 피규어는 일반적인 MoE 구조를 설명하며, Gating Network가 어떻게 특정 전문가를 선택하는지 보여준다.*

> **피규어 캡처: 일반적인 MoE 구조 및 Gating Mechanism**



### 2. DeepSeek-V3의 MoE 개선점

DeepSeek-V3의 MoE는 기존의 MoE와 비교했을 때 다음과 같은 두 가지 주요 차별점을 가진다.

#### (1) Auxiliary-Loss Free Strategy

기존 MoE 모델에서는 특정 전문가(Expert)만 지나치게 활성화되는 현상이 발생하는데, 이를 방지하기 위해 보통 **auxiliary loss**를 추가로 적용하여 균형을 맞춘다. 하지만 auxiliary loss는 **추가적인 계산 비용이 필요**하고, 최적의 loss balancing을 찾는 것이 까다롭다는 단점이 있다.

DeepSeek-V3는 이러한 문제를 해결하기 위해 **auxiliary-loss free strategy**를 도입했다. 즉, **별도의 손실 함수 없이도 전문가 간의 부하를 자동으로 균형 잡도록 설계**했다. 이 방식은 다음과 같은 장점을 가진다.

- **추가적인 loss 계산 없이도 전문가 선택이 자연스럽게 분배됨**
- **Gating Network의 학습이 더 단순해지고, 최적화가 용이해짐**
- **추가적인 hyperparameter 튜닝 없이도 모델의 안정성을 유지**

👉 **[DeepSeek-V3의 MoE 논문 수식 캡처 위치]**
📌 *아래 수식은 auxiliary loss 없이도 전문가 분배를 최적화하는 DeepSeek-V3의 MoE 방식을 설명한다.*

> **논문 수식 캡처: DeepSeek-V3 MoE의 Auxiliary-Loss Free Strategy**

#### (2) Default Top-p Gating Strategy

일반적인 MoE에서는 **Top-k 방식**으로 전문가를 선택하는 경우가 많지만, DeepSeek-V3는 **Top-p 방식**을 기본으로 설정하여 전문가 선택을 더욱 효율적으로 만들었다.

- **Top-k 방식**: k개의 전문가를 선택하는 고정된 방식으로, 특정 전문가가 지나치게 자주 선택되는 경향이 있음.
- **Top-p 방식**: 선택 확률이 높은 전문가부터 순서대로 채택하되, 누적 확률이 p 이상이 되면 중단하는 방식.

DeepSeek-V3의 MoE는 **Default Top-p Gating Strategy**를 채택하여,

- **전문가 간의 불균형을 줄이고 특정 전문가가 독점적으로 활성화되는 문제를 방지**
- **보다 유동적인 전문가 선택이 가능하여 데이터에 따라 적절한 전문가를 활용**
- **추가적인 auxiliary loss 없이도 Load Imbalance 문제 해결**

👉 **[DeepSeek-V3의 MoE 코드 캡처 위치]**
📌 *아래 캡처는 MoE Gating Network에서 Default Top-p 방식이 적용된 코드 부분을 보여준다.*

> **코드 캡처: DeepSeek-V3 MoE의 Default Top-p Gating Strategy 구현**



### 3. DeepSeek-V3 MoE의 장점

DeepSeek-V3는 **auxiliary-loss free strategy**와 **default top-p gating**을 적용하여 기존 MoE 대비 다음과 같은 장점을 갖는다.
✅ **불균형 문제 해결** → 특정 전문가가 독점적으로 활성화되는 문제를 방지
✅ **추가적인 loss 없이 안정적 학습** → auxiliary loss를 사용하지 않아 학습이 단순해지고 최적화가 쉬워짐
✅ **연산 비용 절감** → gating의 효율이 증가하여 불필요한 전문가 호출을 최소화
✅ **추론 속도 향상** → 동적인 전문가 선택을 통해 보다 적은 연산량으로 최적의 결과 도출

결과적으로, DeepSeek-V3의 MoE는 **기존 MoE 모델의 한계를 해결하면서도 효율적인 전문가 선택이 가능하도록 설계된 구조**라 할 수 있다.







## Multi-Token Prediction(MTP)

+ MTP는 train 단계에서 많이 쓰는 방법
+ 기존 MTP 방법론들 소개
+ 기존 MTP 피규어 캡처 추가
+ deepseek의 MTP 차별성 소개
+ deepseek MTP 피규어 캡처 추가
+ 이렇게 할 때 장점 설명
+ modeling deepseek는 추론 모델 구성이라 train은 알 수 없음, 그래서 구체적인 구현은 알 수 없음

시작



![MTP](/images/2025-02-19-DeepSeek-V3/deepseek-mtp.png){: .align-center}





# Mixed Precision

+ deepseek는 대중국 gpu 규제로 H100의 다운그레이드 버전인 H800만을 사용
+ H100 vs H800 비교 캡처 추가
+ 그러나 H800도 hopper 아키텍처임
+ 딥시크는 hopper 아키텍처의 특징인 fp8 혼합정밀도 최적화 방식을 최대한 활용

시작





## FP8 mixed precision

+ deepseek는 모델 학습, 중간 저장, 최종 저장에 fp8, bf16, fp32 모두 활용함
+ 이전 ampere 아키텍처 bf16 혼합 정밀도 설명
+ ampere 아키텍터 장점 캡처 추가
+ 기존 mixed precision 방식 설명
+ 기존 mixed precision 방식 캡처 추가
+ blackwell 이전 지금 가장 대중적인 hopper 아키텍처와 fp8 혼합 정밀도 설명
+ hopper 아키텍처 장점 캡처 추가
+ fp8 mixed precision 방식 설명
+ deepseek mixed precision 방식 캡처 추가(기존과 비교 하는 내용 추가 편집할 것)
+ 그래서 효율적으로 학습을 이뤄냄

시작



![mixed-precision](/images/2025-02-19-DeepSeek-V3/mixed-precision.png){: .align-center}



![backprop-0](/images/2025-02-19-DeepSeek-V3/backprop-0.png){: .align-center}



![backprop-1](/images/2025-02-19-DeepSeek-V3/backprop-1.png){: .align-center}



![backprop-2](/images/2025-02-19-DeepSeek-V3/backprop-2.png){: .align-center}



![backprop-3](/images/2025-02-19-DeepSeek-V3/backprop-3.png){: .align-center}



![backprop-4](/images/2025-02-19-DeepSeek-V3/backprop-4.png){: .align-center}



![backprop-5](/images/2025-02-19-DeepSeek-V3/backprop-5.png){: .align-center}



![backprop-6](/images/2025-02-19-DeepSeek-V3/backprop-6.png){: .align-center}



![backprop-7](/images/2025-02-19-DeepSeek-V3/backprop-7.png){: .align-center}



![deepseek-fp8](/images/2025-02-19-DeepSeek-V3/deepseek-fp8.png){: .align-center}



# Hyper-Parameters

+ deepseek의 단계별 모델 트레이닝 하이퍼 파라미터 정리
+ llama의 단계별 모델 트레이닝 하이퍼 파라미터와 비교
+ 인사이트 추가

시작



![deepseek-pretrain-hparams](/images/2025-02-19-DeepSeek-V3/deepseek-pretrain-hparams.png){: .align-center}



![deepseek-long-context-hparams](/images/2025-02-19-DeepSeek-V3/deepseek-long-context-hparams.png){: .align-center}



![deepseek-sft-hparams](/images/2025-02-19-DeepSeek-V3/deepseek-sft-hparams.png){: .align-center}





# 후기

+ deepseek v3를 리뷰하며 적은 자원 내에서 최대한 효율을 뽑아내려다보니 놀라운 연구 성과가 나타남
+ 기존의 잡기술들을 모두 사용하는 것을 넘어서 발전시킴
+ 물론 데이터에 대한 이슈는 있지만 기술의 발전에 기여한 것은 무시되어선 안됨
+ 하나하나 작은 잡기술들의 적용으로 인해 큰 산을 만들어냄
+ 기술도 예술도 큰 자본에서 규모적 발전을 이뤄낸다면, 결핍에서는 혁신을 이뤄낸다

시작

